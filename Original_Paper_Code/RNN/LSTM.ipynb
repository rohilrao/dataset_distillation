{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bfdd7bb3-aa13-420b-af3e-383c9ee948ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Loading AG News Dataset...\n",
      "Loading Word2Vec Embeddings...\n",
      "Preparing datasets and dataloaders...\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|                                                                                                                                    | 0/1875 [00:00<?, ?it/s]/home/IAIS/rrao/Uni Bonn/lab_distill/Original_Paper_Code/RNN/ipykernel_3382461/4262665667.py:34: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
      "  return torch.tensor(embeddings, dtype=torch.float32)\n",
      "Training: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1875/1875 [06:18<00:00,  4.95it/s]\n",
      "Testing: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 119/119 [00:21<00:00,  5.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Train Loss: 2591.5046, Train Accuracy: 25.54%, Test Accuracy: 25.36%\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1875/1875 [06:18<00:00,  4.95it/s]\n",
      "Testing: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 119/119 [00:21<00:00,  5.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/10], Train Loss: 2586.9244, Train Accuracy: 25.91%, Test Accuracy: 25.80%\n",
      "Epoch 3/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1875/1875 [06:17<00:00,  4.96it/s]\n",
      "Testing: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 119/119 [00:21<00:00,  5.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/10], Train Loss: 2585.4294, Train Accuracy: 25.81%, Test Accuracy: 25.75%\n",
      "Epoch 4/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1875/1875 [06:15<00:00,  4.99it/s]\n",
      "Testing: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 119/119 [00:21<00:00,  5.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/10], Train Loss: 2584.2476, Train Accuracy: 25.94%, Test Accuracy: 25.80%\n",
      "Epoch 5/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  40%|████████████████████████████████████████████████▌                                                                         | 746/1875 [02:30<03:47,  4.96it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 104\u001b[0m\n\u001b[1;32m    102\u001b[0m correct \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    103\u001b[0m total \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m--> 104\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m inputs, labels, lengths \u001b[38;5;129;01min\u001b[39;00m tqdm(train_loader, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    105\u001b[0m     inputs, labels \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mto(device), labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    106\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/tqdm/std.py:1178\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1175\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1177\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1178\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1179\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1180\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1181\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    631\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    632\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 633\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    634\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    635\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    636\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py:677\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    675\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    676\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 677\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    678\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    679\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[0;32mIn[1], line 46\u001b[0m, in \u001b[0;36mAGNewsDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx):\n\u001b[1;32m     45\u001b[0m     tokens \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mtokenize(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtexts[idx])\n\u001b[0;32m---> 46\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m \u001b[43mget_word2vec_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m     label \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabels[idx]\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m embeddings, torch\u001b[38;5;241m.\u001b[39mtensor(label, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong)\n",
      "Cell \u001b[0;32mIn[1], line 30\u001b[0m, in \u001b[0;36mget_word2vec_embeddings\u001b[0;34m(tokens)\u001b[0m\n\u001b[1;32m     28\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m tokens:\n\u001b[0;32m---> 30\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mtoken\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mword2vec\u001b[49m:\n\u001b[1;32m     31\u001b[0m         embeddings\u001b[38;5;241m.\u001b[39mappend(word2vec[token])\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/dataset_distill/lib/python3.9/site-packages/gensim/models/keyedvectors.py:649\u001b[0m, in \u001b[0;36mKeyedVectors.__contains__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    648\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__contains__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key):\n\u001b[0;32m--> 649\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhas_index_for\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "from tqdm import tqdm\n",
    "from transformers import BertTokenizer\n",
    "from datasets import load_dataset\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "import gensim.downloader as api\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "# Check if CUDA is available and set the device accordingly\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "print(\"Loading AG News Dataset...\")\n",
    "dataset = load_dataset('ag_news')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def tokenize(text):\n",
    "    return tokenizer(text, padding='max_length', truncation=True, return_tensors='pt', max_length=128)\n",
    "\n",
    "print(\"Loading Word2Vec Embeddings...\")\n",
    "word2vec = api.load('word2vec-google-news-300')\n",
    "\n",
    "def get_word2vec_embeddings(tokens):\n",
    "    embeddings = []\n",
    "    for token in tokens:\n",
    "        if token in word2vec:\n",
    "            embeddings.append(word2vec[token])\n",
    "        else:\n",
    "            embeddings.append(np.zeros(word2vec.vector_size))\n",
    "    return torch.tensor(embeddings, dtype=torch.float32)\n",
    "\n",
    "class AGNewsDataset(data.Dataset):\n",
    "    def __init__(self, texts, labels):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        tokens = tokenizer.tokenize(self.texts[idx])\n",
    "        embeddings = get_word2vec_embeddings(tokens)\n",
    "        label = self.labels[idx]\n",
    "        return embeddings, torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "print(\"Preparing datasets and dataloaders...\")\n",
    "train_texts = [item['text'] for item in dataset['train']]\n",
    "train_labels = [item['label'] for item in dataset['train']]\n",
    "test_texts = [item['text'] for item in dataset['test']]\n",
    "test_labels = [item['label'] for item in dataset['test']]\n",
    "\n",
    "train_dataset = AGNewsDataset(train_texts, train_labels)\n",
    "test_dataset = AGNewsDataset(test_texts, test_labels)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    embeddings = [item[0] for item in batch]\n",
    "    labels = [item[1] for item in batch]\n",
    "    lengths = [len(seq) for seq in embeddings]\n",
    "    padded_embeddings = pad_sequence(embeddings, batch_first=True)\n",
    "    return padded_embeddings, torch.tensor(labels), torch.tensor(lengths)\n",
    "\n",
    "train_loader = data.DataLoader(train_dataset, batch_size=64, shuffle=True, collate_fn=collate_fn)\n",
    "test_loader = data.DataLoader(test_dataset, batch_size=64, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "class SimpleRNNModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(SimpleRNNModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        packed_input = pack_padded_sequence(x, lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
    "        packed_output, _ = self.rnn(packed_input, h0)\n",
    "        output, _ = pad_packed_sequence(packed_output, batch_first=True)\n",
    "        out = self.fc(output[:, -1, :])\n",
    "        return out\n",
    "\n",
    "input_size = word2vec.vector_size\n",
    "hidden_size = 128\n",
    "num_layers = 2\n",
    "num_classes = 4\n",
    "\n",
    "model = SimpleRNNModel(input_size, hidden_size, num_layers, num_classes).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training and Evaluation\n",
    "num_epochs = 10\n",
    "best_accuracy = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for inputs, labels, lengths in tqdm(train_loader, desc=\"Training\"):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs, lengths)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    train_accuracy = 100 * correct / total\n",
    "    \n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels, lengths in tqdm(test_loader, desc=\"Testing\"):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs, lengths)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    test_accuracy = 100 * correct / total\n",
    "    print(f'Epoch [{epoch + 1}/{num_epochs}], Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.2f}%, Test Accuracy: {test_accuracy:.2f}%')\n",
    "    \n",
    "    if test_accuracy > best_accuracy:\n",
    "        best_accuracy = test_accuracy\n",
    "        torch.save(model.state_dict(), 'best_model.pt')\n",
    "\n",
    "# Load the best model\n",
    "print(\"Loading best model...\")\n",
    "model.load_state_dict(torch.load('best_model.pt'))\n",
    "model.to(device)\n",
    "\n",
    "# Generate Classification Report\n",
    "print(\"Generating classification report...\")\n",
    "y_true = []\n",
    "y_pred = []\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for inputs, labels, lengths in tqdm(test_loader, desc=\"Evaluating\"):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model(inputs, lengths)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        y_true.extend(labels.tolist())\n",
    "        y_pred.extend(predicted.tolist())\n",
    "\n",
    "print(classification_report(y_true, y_pred, target_names=dataset['test'].features['label'].names))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70cd948d-8a80-4d5a-add0-4d0d1f1fdd20",
   "metadata": {},
   "outputs": [],
   "source": [
    "!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "94e3edf2-5576-4021-92dc-2e7932e8590c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Word2Vec embeddings...\n",
      "Using device: cuda\n",
      "Loading AG_NEWS dataset...\n",
      "Building vocabulary...\n",
      "Creating embeddings matrix...\n",
      "Starting training...\n",
      "Epoch: 1, Batch: 0, Loss: 1.3962640762329102\n",
      "Epoch: 1, Batch: 100, Loss: 1.3789664506912231\n",
      "Epoch: 1, Batch: 200, Loss: 1.3892250061035156\n",
      "Epoch: 1, Batch: 300, Loss: 1.3963209390640259\n",
      "Epoch: 1, Batch: 400, Loss: 1.3627750873565674\n",
      "Epoch: 1, Batch: 500, Loss: 1.2390058040618896\n",
      "Epoch: 1, Batch: 600, Loss: 1.2338005304336548\n",
      "Epoch: 1, Batch: 700, Loss: 1.2921500205993652\n",
      "Epoch: 1, Batch: 800, Loss: 1.2750242948532104\n",
      "Epoch: 1, Batch: 900, Loss: 1.2657489776611328\n",
      "Epoch: 1, Batch: 1000, Loss: 1.27080500125885\n",
      "Epoch: 1, Batch: 1100, Loss: 1.2660959959030151\n",
      "Epoch: 1, Batch: 1200, Loss: 1.3710660934448242\n",
      "Epoch: 1, Batch: 1300, Loss: 1.3817782402038574\n",
      "Epoch: 1, Batch: 1400, Loss: 1.3726297616958618\n",
      "Epoch: 1, Batch: 1500, Loss: 1.3053982257843018\n",
      "Epoch: 1, Batch: 1600, Loss: 1.4326967000961304\n",
      "Epoch: 1, Batch: 1700, Loss: 1.3224880695343018\n",
      "Epoch: 1, Batch: 1800, Loss: 1.240360140800476\n",
      "Epoch: 1, Batch: 1900, Loss: 0.9800487160682678\n",
      "Epoch: 1, Batch: 2000, Loss: 0.8422547578811646\n",
      "Epoch: 1, Batch: 2100, Loss: 0.5732444524765015\n",
      "Epoch: 1, Batch: 2200, Loss: 0.5546755194664001\n",
      "Epoch: 1, Batch: 2300, Loss: 0.5277896523475647\n",
      "Epoch: 1, Batch: 2400, Loss: 0.7645054459571838\n",
      "Epoch: 1, Batch: 2500, Loss: 0.4940354824066162\n",
      "Epoch: 1, Batch: 2600, Loss: 0.6511187553405762\n",
      "Epoch: 1, Batch: 2700, Loss: 0.5774896144866943\n",
      "Epoch: 1, Batch: 2800, Loss: 0.5707723498344421\n",
      "Epoch: 1, Batch: 2900, Loss: 0.48939311504364014\n",
      "Epoch: 1, Batch: 3000, Loss: 0.14484992623329163\n",
      "Epoch: 1, Batch: 3100, Loss: 0.3797042965888977\n",
      "Epoch: 1, Batch: 3200, Loss: 0.6492271423339844\n",
      "Epoch: 1, Batch: 3300, Loss: 0.2771708071231842\n",
      "Epoch: 1, Batch: 3400, Loss: 0.3539979159832001\n",
      "Epoch: 1, Batch: 3500, Loss: 0.45759260654449463\n",
      "Epoch: 1, Batch: 3600, Loss: 0.2355983704328537\n",
      "Epoch: 1, Batch: 3700, Loss: 0.42079949378967285\n",
      "Epoch: 1, Average Loss: 0.888007912282149\n",
      "Evaluating the model...\n",
      "Test Accuracy: 0.9007\n",
      "Epoch: 2, Batch: 0, Loss: 0.23363080620765686\n",
      "Epoch: 2, Batch: 100, Loss: 0.12256348878145218\n",
      "Epoch: 2, Batch: 200, Loss: 0.24424123764038086\n",
      "Epoch: 2, Batch: 300, Loss: 0.401878297328949\n",
      "Epoch: 2, Batch: 400, Loss: 0.29743772745132446\n",
      "Epoch: 2, Batch: 500, Loss: 0.3194774389266968\n",
      "Epoch: 2, Batch: 600, Loss: 0.3113477826118469\n",
      "Epoch: 2, Batch: 700, Loss: 0.14075630903244019\n",
      "Epoch: 2, Batch: 800, Loss: 0.5147296786308289\n",
      "Epoch: 2, Batch: 900, Loss: 0.15446719527244568\n",
      "Epoch: 2, Batch: 1000, Loss: 0.0676613375544548\n",
      "Epoch: 2, Batch: 1100, Loss: 0.10036278516054153\n",
      "Epoch: 2, Batch: 1200, Loss: 0.3102468252182007\n",
      "Epoch: 2, Batch: 1300, Loss: 0.16422811150550842\n",
      "Epoch: 2, Batch: 1400, Loss: 0.1488366276025772\n",
      "Epoch: 2, Batch: 1500, Loss: 0.31003427505493164\n",
      "Epoch: 2, Batch: 1600, Loss: 0.2924683690071106\n",
      "Epoch: 2, Batch: 1700, Loss: 0.1356779932975769\n",
      "Epoch: 2, Batch: 1800, Loss: 0.24431635439395905\n",
      "Epoch: 2, Batch: 1900, Loss: 0.3754710555076599\n",
      "Epoch: 2, Batch: 2000, Loss: 0.11893703043460846\n",
      "Epoch: 2, Batch: 2100, Loss: 0.46618911623954773\n",
      "Epoch: 2, Batch: 2200, Loss: 0.1346644014120102\n",
      "Epoch: 2, Batch: 2300, Loss: 0.33883610367774963\n",
      "Epoch: 2, Batch: 2400, Loss: 0.41532981395721436\n",
      "Epoch: 2, Batch: 2500, Loss: 0.3085387349128723\n",
      "Epoch: 2, Batch: 2600, Loss: 0.07540209591388702\n",
      "Epoch: 2, Batch: 2700, Loss: 0.261332243680954\n",
      "Epoch: 2, Batch: 2800, Loss: 0.48216748237609863\n",
      "Epoch: 2, Batch: 2900, Loss: 0.3307304382324219\n",
      "Epoch: 2, Batch: 3000, Loss: 0.4857446551322937\n",
      "Epoch: 2, Batch: 3100, Loss: 0.16011285781860352\n",
      "Epoch: 2, Batch: 3200, Loss: 0.06910547614097595\n",
      "Epoch: 2, Batch: 3300, Loss: 0.1787889003753662\n",
      "Epoch: 2, Batch: 3400, Loss: 0.09341778606176376\n",
      "Epoch: 2, Batch: 3500, Loss: 0.15609309077262878\n",
      "Epoch: 2, Batch: 3600, Loss: 0.22595533728599548\n",
      "Epoch: 2, Batch: 3700, Loss: 0.08020631223917007\n",
      "Epoch: 2, Average Loss: 0.24928229203770558\n",
      "Evaluating the model...\n",
      "Test Accuracy: 0.9154\n",
      "Epoch: 3, Batch: 0, Loss: 0.08876270800828934\n",
      "Epoch: 3, Batch: 100, Loss: 0.12104158103466034\n",
      "Epoch: 3, Batch: 200, Loss: 0.16118024289608002\n",
      "Epoch: 3, Batch: 300, Loss: 0.02971288375556469\n",
      "Epoch: 3, Batch: 400, Loss: 0.20214474201202393\n",
      "Epoch: 3, Batch: 500, Loss: 0.2655820846557617\n",
      "Epoch: 3, Batch: 600, Loss: 0.02070564776659012\n",
      "Epoch: 3, Batch: 700, Loss: 0.19971084594726562\n",
      "Epoch: 3, Batch: 800, Loss: 0.07693208009004593\n",
      "Epoch: 3, Batch: 900, Loss: 0.3171130120754242\n",
      "Epoch: 3, Batch: 1000, Loss: 0.1536514312028885\n",
      "Epoch: 3, Batch: 1100, Loss: 0.26538121700286865\n",
      "Epoch: 3, Batch: 1200, Loss: 0.028131717815995216\n",
      "Epoch: 3, Batch: 1300, Loss: 0.32365894317626953\n",
      "Epoch: 3, Batch: 1400, Loss: 0.16881290078163147\n",
      "Epoch: 3, Batch: 1500, Loss: 0.13131676614284515\n",
      "Epoch: 3, Batch: 1600, Loss: 0.08921840041875839\n",
      "Epoch: 3, Batch: 1700, Loss: 0.03137374296784401\n",
      "Epoch: 3, Batch: 1800, Loss: 0.39898210763931274\n",
      "Epoch: 3, Batch: 1900, Loss: 0.5745380520820618\n",
      "Epoch: 3, Batch: 2000, Loss: 0.02623920701444149\n",
      "Epoch: 3, Batch: 2100, Loss: 0.16554509103298187\n",
      "Epoch: 3, Batch: 2200, Loss: 0.21234413981437683\n",
      "Epoch: 3, Batch: 2300, Loss: 0.22639258205890656\n",
      "Epoch: 3, Batch: 2400, Loss: 0.05939299613237381\n",
      "Epoch: 3, Batch: 2500, Loss: 0.29670315980911255\n",
      "Epoch: 3, Batch: 2600, Loss: 0.12001710385084152\n",
      "Epoch: 3, Batch: 2700, Loss: 0.041670020669698715\n",
      "Epoch: 3, Batch: 2800, Loss: 0.09717497229576111\n",
      "Epoch: 3, Batch: 2900, Loss: 0.17565137147903442\n",
      "Epoch: 3, Batch: 3000, Loss: 0.31868624687194824\n",
      "Epoch: 3, Batch: 3100, Loss: 0.041074879467487335\n",
      "Epoch: 3, Batch: 3200, Loss: 0.029923617839813232\n",
      "Epoch: 3, Batch: 3300, Loss: 0.08004461973905563\n",
      "Epoch: 3, Batch: 3400, Loss: 0.22210216522216797\n",
      "Epoch: 3, Batch: 3500, Loss: 0.0766332671046257\n",
      "Epoch: 3, Batch: 3600, Loss: 0.18583598732948303\n",
      "Epoch: 3, Batch: 3700, Loss: 0.05474592000246048\n",
      "Epoch: 3, Average Loss: 0.14981744163880745\n",
      "Evaluating the model...\n",
      "Test Accuracy: 0.9209\n",
      "Epoch: 4, Batch: 0, Loss: 0.021513916552066803\n",
      "Epoch: 4, Batch: 100, Loss: 0.10098403692245483\n",
      "Epoch: 4, Batch: 200, Loss: 0.03805870562791824\n",
      "Epoch: 4, Batch: 300, Loss: 0.1642080843448639\n",
      "Epoch: 4, Batch: 400, Loss: 0.10081441700458527\n",
      "Epoch: 4, Batch: 500, Loss: 0.036064304411411285\n",
      "Epoch: 4, Batch: 600, Loss: 0.13825607299804688\n",
      "Epoch: 4, Batch: 700, Loss: 0.014059757813811302\n",
      "Epoch: 4, Batch: 800, Loss: 0.02555198222398758\n",
      "Epoch: 4, Batch: 900, Loss: 0.08253896981477737\n",
      "Epoch: 4, Batch: 1000, Loss: 0.05229679122567177\n",
      "Epoch: 4, Batch: 1100, Loss: 0.12585237622261047\n",
      "Epoch: 4, Batch: 1200, Loss: 0.012243981473147869\n",
      "Epoch: 4, Batch: 1300, Loss: 0.01900537870824337\n",
      "Epoch: 4, Batch: 1400, Loss: 0.15668737888336182\n",
      "Epoch: 4, Batch: 1500, Loss: 0.007061525713652372\n",
      "Epoch: 4, Batch: 1600, Loss: 0.014538965187966824\n",
      "Epoch: 4, Batch: 1700, Loss: 0.018106995150446892\n",
      "Epoch: 4, Batch: 1800, Loss: 0.21308618783950806\n",
      "Epoch: 4, Batch: 1900, Loss: 0.2706882059574127\n",
      "Epoch: 4, Batch: 2000, Loss: 0.2378947138786316\n",
      "Epoch: 4, Batch: 2100, Loss: 0.0029839822091162205\n",
      "Epoch: 4, Batch: 2200, Loss: 0.03797800466418266\n",
      "Epoch: 4, Batch: 2300, Loss: 0.06467445939779282\n",
      "Epoch: 4, Batch: 2400, Loss: 0.004174353554844856\n",
      "Epoch: 4, Batch: 2500, Loss: 0.1492922455072403\n",
      "Epoch: 4, Batch: 2600, Loss: 0.17676515877246857\n",
      "Epoch: 4, Batch: 2700, Loss: 0.01973767764866352\n",
      "Epoch: 4, Batch: 2800, Loss: 0.04655296728014946\n",
      "Epoch: 4, Batch: 2900, Loss: 0.3769538402557373\n",
      "Epoch: 4, Batch: 3000, Loss: 0.008908608928322792\n",
      "Epoch: 4, Batch: 3100, Loss: 0.08069581538438797\n",
      "Epoch: 4, Batch: 3200, Loss: 0.11313280463218689\n",
      "Epoch: 4, Batch: 3300, Loss: 0.01156691461801529\n",
      "Epoch: 4, Batch: 3400, Loss: 0.03556934744119644\n",
      "Epoch: 4, Batch: 3500, Loss: 0.18621478974819183\n",
      "Epoch: 4, Batch: 3600, Loss: 0.02271066978573799\n",
      "Epoch: 4, Batch: 3700, Loss: 0.0359652005136013\n",
      "Epoch: 4, Average Loss: 0.08707689801088224\n",
      "Evaluating the model...\n",
      "Test Accuracy: 0.9167\n",
      "Epoch: 5, Batch: 0, Loss: 0.005258586257696152\n",
      "Epoch: 5, Batch: 100, Loss: 0.004681786056607962\n",
      "Epoch: 5, Batch: 200, Loss: 0.00789505522698164\n",
      "Epoch: 5, Batch: 300, Loss: 0.024110183119773865\n",
      "Epoch: 5, Batch: 400, Loss: 0.16719014942646027\n",
      "Epoch: 5, Batch: 500, Loss: 0.044893018901348114\n",
      "Epoch: 5, Batch: 600, Loss: 0.012879028916358948\n",
      "Epoch: 5, Batch: 700, Loss: 0.0033616269938647747\n",
      "Epoch: 5, Batch: 800, Loss: 0.11740865558385849\n",
      "Epoch: 5, Batch: 900, Loss: 0.05734025314450264\n",
      "Epoch: 5, Batch: 1000, Loss: 0.01717519946396351\n",
      "Epoch: 5, Batch: 1100, Loss: 0.05895736441016197\n",
      "Epoch: 5, Batch: 1200, Loss: 0.16736817359924316\n",
      "Epoch: 5, Batch: 1300, Loss: 0.0017505252035334706\n",
      "Epoch: 5, Batch: 1400, Loss: 0.0277907382696867\n",
      "Epoch: 5, Batch: 1500, Loss: 0.004906832240521908\n",
      "Epoch: 5, Batch: 1600, Loss: 0.007642802316695452\n",
      "Epoch: 5, Batch: 1700, Loss: 0.17645229399204254\n",
      "Epoch: 5, Batch: 1800, Loss: 0.008954860270023346\n",
      "Epoch: 5, Batch: 1900, Loss: 0.039235204458236694\n",
      "Epoch: 5, Batch: 2000, Loss: 0.07468144595623016\n",
      "Epoch: 5, Batch: 2100, Loss: 0.0030148508958518505\n",
      "Epoch: 5, Batch: 2200, Loss: 0.07143757492303848\n",
      "Epoch: 5, Batch: 2300, Loss: 0.021528664976358414\n",
      "Epoch: 5, Batch: 2400, Loss: 0.00453623291105032\n",
      "Epoch: 5, Batch: 2500, Loss: 0.029743419960141182\n",
      "Epoch: 5, Batch: 2600, Loss: 0.011424006894230843\n",
      "Epoch: 5, Batch: 2700, Loss: 0.027988184243440628\n",
      "Epoch: 5, Batch: 2800, Loss: 0.021558234468102455\n",
      "Epoch: 5, Batch: 2900, Loss: 0.2595347464084625\n",
      "Epoch: 5, Batch: 3000, Loss: 0.007741501554846764\n",
      "Epoch: 5, Batch: 3100, Loss: 0.03133426234126091\n",
      "Epoch: 5, Batch: 3200, Loss: 0.02950439229607582\n",
      "Epoch: 5, Batch: 3300, Loss: 0.004216965287923813\n",
      "Epoch: 5, Batch: 3400, Loss: 0.035189028829336166\n",
      "Epoch: 5, Batch: 3500, Loss: 0.00799585971981287\n",
      "Epoch: 5, Batch: 3600, Loss: 0.023407913744449615\n",
      "Epoch: 5, Batch: 3700, Loss: 0.01728205941617489\n",
      "Epoch: 5, Average Loss: 0.05331965273457269\n",
      "Evaluating the model...\n",
      "Test Accuracy: 0.9133\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import gensim.downloader as api\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from datasets import load_dataset\n",
    "\n",
    "print(\"Loading Word2Vec embeddings...\")\n",
    "word2vec = api.load('word2vec-google-news-300')\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load AG_NEWS dataset using Hugging Face datasets library\n",
    "print(\"Loading AG_NEWS dataset...\")\n",
    "dataset = load_dataset('ag_news')\n",
    "\n",
    "# Tokenizer function\n",
    "def tokenize(text):\n",
    "    return text.lower().split()\n",
    "\n",
    "# Build vocabulary\n",
    "def build_vocab(texts, tokenizer):\n",
    "    vocab = {\"<unk>\": 0}\n",
    "    for text in texts:\n",
    "        for token in tokenizer(text):\n",
    "            if token not in vocab:\n",
    "                vocab[token] = len(vocab)\n",
    "    return vocab\n",
    "\n",
    "# Custom Dataset class\n",
    "class AGNewsDataset(Dataset):\n",
    "    def __init__(self, texts, labels, vocab, tokenizer):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.vocab = vocab\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        tokenized_text = [self.vocab[token] if token in self.vocab else self.vocab[\"<unk>\"]\n",
    "                          for token in self.tokenizer(text)]\n",
    "        return torch.tensor(tokenized_text, dtype=torch.long), torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "def collate_batch(batch):\n",
    "    label_list, text_list = [], []\n",
    "    for _text, _label in batch:\n",
    "        label_list.append(_label)\n",
    "        text_list.append(_text)\n",
    "    text_list = pad_sequence(text_list, batch_first=True, padding_value=0)\n",
    "    label_list = torch.tensor(label_list, dtype=torch.long)\n",
    "    return text_list.to(device), label_list.to(device)\n",
    "\n",
    "# Extract texts and labels\n",
    "train_texts = [item['text'] for item in dataset['train']]\n",
    "train_labels = [item['label'] for item in dataset['train']]\n",
    "test_texts = [item['text'] for item in dataset['test']]\n",
    "test_labels = [item['label'] for item in dataset['test']]\n",
    "\n",
    "# Build vocabulary from training texts\n",
    "print(\"Building vocabulary...\")\n",
    "vocab = build_vocab(train_texts, tokenize)\n",
    "\n",
    "# Create Dataset and DataLoader\n",
    "train_dataset = AGNewsDataset(train_texts, train_labels, vocab, tokenize)\n",
    "test_dataset = AGNewsDataset(test_texts, test_labels, vocab, tokenize)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_batch)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, collate_fn=collate_batch)\n",
    "\n",
    "# Create embedding matrix\n",
    "print(\"Creating embeddings matrix...\")\n",
    "embedding_dim = 300\n",
    "embedding_matrix = np.zeros((len(vocab), embedding_dim))\n",
    "for word, idx in vocab.items():\n",
    "    if word in word2vec:\n",
    "        embedding_matrix[idx] = word2vec[word]\n",
    "    else:\n",
    "        embedding_matrix[idx] = np.random.normal(scale=0.6, size=(embedding_dim,))\n",
    "\n",
    "embedding_matrix = torch.tensor(embedding_matrix, dtype=torch.float32).to(device)\n",
    "\n",
    "# Define the LSTM model\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, embedding_matrix, hidden_dim, output_dim, n_layers, drop_prob=0.5):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        num_embeddings, embedding_dim = embedding_matrix.shape\n",
    "        self.embedding = nn.Embedding.from_pretrained(embedding_matrix, freeze=False)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=n_layers, dropout=drop_prob, batch_first=True)\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        lstm_out, (ht, ct) = self.lstm(x)\n",
    "        out = self.dropout(ht[-1])\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "hidden_dim = 256\n",
    "output_dim = 4  # AG_NEWS has 4 classes\n",
    "n_layers = 2\n",
    "model = LSTMClassifier(embedding_matrix, hidden_dim, output_dim, n_layers).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training the model\n",
    "print(\"Starting training...\")\n",
    "num_epochs = 5\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for idx, (text, labels) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(text)\n",
    "        loss = criterion(output, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        if idx % 100 == 0:\n",
    "            print(f'Epoch: {epoch+1}, Batch: {idx}, Loss: {loss.item()}')\n",
    "    print(f'Epoch: {epoch+1}, Average Loss: {total_loss/len(train_loader)}')\n",
    "\n",
    "    # Evaluate the model\n",
    "    print(\"Evaluating the model...\")\n",
    "    model.eval()\n",
    "    total_acc, total_count = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for idx, (text, labels) in enumerate(test_loader):\n",
    "            output = model(text)\n",
    "            _, predicted = torch.max(output, 1)\n",
    "            total_acc += (predicted == labels).sum().item()\n",
    "            total_count += labels.size(0)\n",
    "        print(f'Test Accuracy: {total_acc/total_count:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e1617359-9b49-4e91-89de-70d6d4426c4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 17872, 125152,  18559,  15120, 122869, 122868,  14556,   9574,     15,\n",
       "           208,  18073,   9174,     39,      7,    465,   2379,     93,   2157,\n",
       "             7,  17876,  17872,     93,     88, 158508,   2268,     62,      7,\n",
       "          7767, 107695,    380,  11381,    736,   1774,  68858,   7254,     93,\n",
       "            97,   6492,   1878,    495,      0,      0,      0,      0,      0,\n",
       "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "             0,      0,      0], device='cuda:0')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2c41aef6-ca07-4c21-9e34-c18d68f0e097",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, embedding_matrix, hidden_dim, output_dim, n_layers, drop_prob=0.5):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        num_embeddings, embedding_dim = embedding_matrix.shape\n",
    "        self.embedding = nn.Embedding.from_pretrained(embedding_matrix, freeze=False)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=n_layers, dropout=drop_prob, batch_first=True)\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        print(x.shape)\n",
    "        lstm_out, (ht, ct) = self.lstm(x)\n",
    "        out = self.dropout(ht[-1])\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6f9f52ca-9ddf-4257-b810-3b25c8c70255",
   "metadata": {},
   "outputs": [],
   "source": [
    "m1 = LSTMClassifier(embedding_matrix, hidden_dim, output_dim, n_layers).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c8af170f-8414-40fc-80ca-0e025021873d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 66, 300])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0170, -0.0626,  0.0580,  0.0675],\n",
       "        [ 0.0302, -0.0568,  0.0816,  0.0866],\n",
       "        [ 0.0496,  0.0877,  0.0744,  0.1482],\n",
       "        [ 0.0785,  0.0370, -0.0112,  0.0976],\n",
       "        [-0.0072, -0.0847,  0.0542,  0.0955],\n",
       "        [ 0.0820, -0.0085,  0.0478,  0.1129],\n",
       "        [ 0.0838,  0.0043, -0.0123,  0.0477],\n",
       "        [ 0.0580, -0.0325, -0.0113,  0.0906],\n",
       "        [ 0.0537, -0.0645,  0.0122,  0.1611],\n",
       "        [ 0.0893, -0.0263,  0.0266,  0.1330],\n",
       "        [ 0.0778, -0.0523, -0.0048,  0.0621],\n",
       "        [ 0.1100, -0.0733, -0.0012,  0.1339],\n",
       "        [ 0.0419, -0.0561,  0.0796,  0.0509],\n",
       "        [ 0.0657, -0.0634,  0.0956,  0.1878],\n",
       "        [ 0.0790, -0.0569,  0.0845,  0.1133],\n",
       "        [ 0.0727, -0.0475,  0.0944,  0.1565]], device='cuda:0',\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m1(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5da5d1a-e9e8-42f4-977d-e3a7472949a9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
