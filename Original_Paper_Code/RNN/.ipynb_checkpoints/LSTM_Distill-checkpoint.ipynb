{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c5bd112c-fc73-437e-8802-b562a08d4305",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Word2Vec embeddings...\n",
      "Using device: cuda\n",
      "Loading AG_NEWS dataset...\n",
      "Building vocabulary...\n",
      "Creating embeddings matrix...\n",
      "\n",
      "================== Exp 0 ==================\n",
      "\n",
      "Training begins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1/10: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 106.70batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Train Loss: 2.8262, Train Accuracy: 17.50%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2/10: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 105.09batch/s]\n",
      "Training Epoch 3/10: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 104.23batch/s]\n",
      "Training Epoch 4/10: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 105.34batch/s]\n",
      "Training Epoch 5/10: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 105.16batch/s]\n",
      "Training Epoch 6/10: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 98.76batch/s]\n",
      "Training Epoch 7/10: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 94.29batch/s]\n",
      "Training Epoch 8/10: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 91.11batch/s]\n",
      "Training Epoch 9/10: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 86.38batch/s]\n",
      "Training Epoch 10/10: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 88.67batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/10], Train Loss: 2.8152, Train Accuracy: 32.50%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating on test set: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████| 238/238 [00:01<00:00, 194.55batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.2537\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                 | 0/1 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting outer loop\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument index in method wrapper_CUDA__index_select)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 232\u001b[0m\n\u001b[1;32m    230\u001b[0m text_syn \u001b[38;5;241m=\u001b[39m synthetic_text_data[c \u001b[38;5;241m*\u001b[39m n_spc:(c \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m*\u001b[39m n_spc]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    231\u001b[0m lab_syn \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mones((n_spc,), device\u001b[38;5;241m=\u001b[39mdevice, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong) \u001b[38;5;241m*\u001b[39m c\n\u001b[0;32m--> 232\u001b[0m output_real \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext_real\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    233\u001b[0m loss_real \u001b[38;5;241m=\u001b[39m criterion(output_real, lab_real)\n\u001b[1;32m    234\u001b[0m gw_real \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mgrad(loss_real, model_parameters)\n",
      "File \u001b[0;32m~/anaconda3/envs/dataset_distill/lib/python3.9/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/dataset_distill/lib/python3.9/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[8], line 108\u001b[0m, in \u001b[0;36mLSTMClassifier.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m--> 108\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    109\u001b[0m     lstm_out, (ht, ct) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlstm(x)\n\u001b[1;32m    110\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(ht[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n",
      "File \u001b[0;32m~/anaconda3/envs/dataset_distill/lib/python3.9/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/dataset_distill/lib/python3.9/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/dataset_distill/lib/python3.9/site-packages/torch/nn/modules/sparse.py:164\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 164\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/dataset_distill/lib/python3.9/site-packages/torch/nn/functional.py:2267\u001b[0m, in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2261\u001b[0m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[1;32m   2262\u001b[0m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[1;32m   2263\u001b[0m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[1;32m   2264\u001b[0m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[1;32m   2265\u001b[0m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[1;32m   2266\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[0;32m-> 2267\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument index in method wrapper_CUDA__index_select)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import gensim.downloader as api\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "print(\"Loading Word2Vec embeddings...\")\n",
    "word2vec = api.load('word2vec-google-news-300')\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load AG_NEWS dataset using Hugging Face datasets library\n",
    "print(\"Loading AG_NEWS dataset...\")\n",
    "dataset = load_dataset('ag_news')\n",
    "\n",
    "# Tokenizer function\n",
    "def tokenize(text):\n",
    "    return text.lower().split()\n",
    "\n",
    "# Build vocabulary\n",
    "def build_vocab(texts, tokenizer):\n",
    "    vocab = {\"<unk>\": 0}\n",
    "    for text in texts:\n",
    "        for token in tokenizer(text):\n",
    "            if token not in vocab:\n",
    "                vocab[token] = len(vocab)\n",
    "    return vocab\n",
    "\n",
    "# Custom Dataset class\n",
    "class AGNewsDataset(Dataset):\n",
    "    def __init__(self, texts, labels, vocab, tokenizer):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.vocab = vocab\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        tokenized_text = [self.vocab[token] if token in self.vocab else self.vocab[\"<unk>\"]\n",
    "                          for token in self.tokenizer(text)]\n",
    "        return torch.tensor(tokenized_text, dtype=torch.long), torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "def pad_sequences(sequences, max_len=None, pad_value=0):\n",
    "    if not max_len:\n",
    "        max_len = max(len(seq) for seq in sequences)\n",
    "    padded_seqs = torch.full((len(sequences), max_len), pad_value, dtype=torch.long)\n",
    "    for i, seq in enumerate(sequences):\n",
    "        length = len(seq)\n",
    "        padded_seqs[i, :length] = seq\n",
    "    return padded_seqs\n",
    "\n",
    "# Extract texts and labels\n",
    "train_texts = [item['text'] for item in dataset['train']]\n",
    "train_labels = [item['label'] for item in dataset['train']]\n",
    "test_texts = [item['text'] for item in dataset['test']]\n",
    "test_labels = [item['label'] for item in dataset['test']]\n",
    "\n",
    "# Build vocabulary from training texts\n",
    "print(\"Building vocabulary...\")\n",
    "vocab = build_vocab(train_texts, tokenize)\n",
    "\n",
    "# Create Dataset and DataLoader\n",
    "train_dataset = AGNewsDataset(train_texts, train_labels, vocab, tokenize)\n",
    "test_dataset = AGNewsDataset(test_texts, test_labels, vocab, tokenize)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    texts, labels = zip(*batch)\n",
    "    texts_padded = pad_sequences(texts)\n",
    "    labels = torch.tensor(labels, dtype=torch.long)\n",
    "    return texts_padded, labels\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "# Create embedding matrix\n",
    "print(\"Creating embeddings matrix...\")\n",
    "embedding_dim = 300\n",
    "embedding_matrix = np.zeros((len(vocab), embedding_dim))\n",
    "for word, idx in vocab.items():\n",
    "    if word in word2vec:\n",
    "        embedding_matrix[idx] = word2vec[word]\n",
    "    else:\n",
    "        embedding_matrix[idx] = np.random.normal(scale=0.6, size=(embedding_dim,))\n",
    "\n",
    "embedding_matrix = torch.tensor(embedding_matrix, dtype=torch.float32).to(device)\n",
    "\n",
    "# Define the LSTM model\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, embedding_matrix, hidden_dim, output_dim, n_layers, drop_prob=0.5):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        num_embeddings, embedding_dim = embedding_matrix.shape\n",
    "        self.embedding = nn.Embedding.from_pretrained(embedding_matrix, freeze=True)  # Freeze embeddings\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=n_layers, dropout=drop_prob, batch_first=True)\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        lstm_out, (ht, ct) = self.lstm(x)\n",
    "        out = self.dropout(ht[-1])\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "hidden_dim = 256\n",
    "output_dim = 4  # AG_NEWS has 4 classes\n",
    "n_layers = 2\n",
    "model = LSTMClassifier(embedding_matrix, hidden_dim, output_dim, n_layers).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "def evaluate_syn(model, synthetic_text_data, synthetic_labels, test_loader, device, num_epochs=10, lr=0.001):\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer_net = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    synthetic_dataset = DataLoader(torch.utils.data.TensorDataset(synthetic_text_data.detach(), synthetic_labels), batch_size=32, shuffle=True)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for inputs, labels in tqdm(synthetic_dataset, desc=f'Training Epoch {epoch+1}/{num_epochs}', unit='batch'):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer_net.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer_net.step()\n",
    "            train_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "        train_accuracy = 100 * correct / total\n",
    "        if epoch % 9 == 0:\n",
    "            print(f'Epoch [{epoch + 1}/{num_epochs}], Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.2f}%')\n",
    "\n",
    "    model.eval()\n",
    "    total_acc, total_count = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for text, labels in tqdm(test_loader, desc='Evaluating on test set', unit='batch'):\n",
    "            text, labels = text.to(device), labels.to(device)\n",
    "            output = model(text)\n",
    "            _, predicted = torch.max(output, 1)\n",
    "            total_acc += (predicted == labels).sum().item()\n",
    "            total_count += labels.size(0)\n",
    "\n",
    "    overall_accuracy = total_acc / total_count\n",
    "    print(f'Test Accuracy: {overall_accuracy:.4f}')\n",
    "\n",
    "    results = {\n",
    "        'test_accuracy': overall_accuracy,\n",
    "    }\n",
    "    with open('results.pkl', 'wb') as f:\n",
    "        pickle.dump(results, f)\n",
    "\n",
    "    return overall_accuracy\n",
    "\n",
    "n_experiments = 1\n",
    "num_classes = len(set(train_labels))\n",
    "n_spc = 10  # samples_per_class\n",
    "n_iterations = 1000\n",
    "batch_train = 32\n",
    "batch_real = 32\n",
    "batch_syn = 32\n",
    "n_outer_loop = 1\n",
    "n_inner_loop = 1\n",
    "num_eval_epochs = 20\n",
    "\n",
    "data_save = []\n",
    "accs_hist = []\n",
    "\n",
    "def get_class_indices(labels, n_classes):\n",
    "    class_indices = [[] for _ in range(n_classes)]\n",
    "    for idx, label in enumerate(labels):\n",
    "        class_indices[label].append(idx)\n",
    "    return class_indices\n",
    "\n",
    "train_labels_list = [item['label'] for item in dataset['train']]\n",
    "class_indices = get_class_indices(train_labels_list, num_classes)\n",
    "\n",
    "for exp_n in range(n_experiments):\n",
    "    print(f'\\n================== Exp {exp_n} ==================\\n')\n",
    "\n",
    "    # Randomly select synthetic data from train dataset per class\n",
    "    synthetic_text_data = []\n",
    "    synthetic_labels = []\n",
    "    for class_idx in range(num_classes):\n",
    "        selected_indices = random.sample(class_indices[class_idx], n_spc)\n",
    "        for idx in selected_indices:\n",
    "            synthetic_text_data.append(train_dataset[idx][0])\n",
    "            synthetic_labels.append(train_dataset[idx][1])\n",
    "    synthetic_text_data = pad_sequences(synthetic_text_data).to(device)\n",
    "    synthetic_labels = torch.tensor(synthetic_labels, dtype=torch.long).to(device)\n",
    "\n",
    "    optimizer_syn = optim.SGD([synthetic_text_data], lr=0.01, momentum=0.9)\n",
    "    optimizer_syn.zero_grad()\n",
    "    criterion = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "    print('Training begins')\n",
    "\n",
    "    for it in range(n_iterations + 1):\n",
    "        model = LSTMClassifier(embedding_matrix, hidden_dim, output_dim, n_layers).to(device)\n",
    "        model.train()\n",
    "        model_parameters = list(model.parameters())\n",
    "        optimizer_net = optim.SGD(model.parameters(), lr=0.01)\n",
    "        optimizer_net.zero_grad()\n",
    "        loss_avg = 0\n",
    "\n",
    "        accs_hist.append(evaluate_syn(model, synthetic_text_data, synthetic_labels, test_loader, device, num_epochs=10, lr=0.001))\n",
    "\n",
    "        for ol in tqdm(range(n_outer_loop)):\n",
    "            print('Starting outer loop')\n",
    "            loss = torch.tensor(0.0).to(device)\n",
    "\n",
    "            for c in range(num_classes):\n",
    "                text_real = next(iter(train_loader))[0]\n",
    "                lab_real = torch.ones((text_real.shape[0],), device=device, dtype=torch.long) * c\n",
    "                text_syn = synthetic_text_data[c * n_spc:(c + 1) * n_spc].to(device)\n",
    "                lab_syn = torch.ones((n_spc,), device=device, dtype=torch.long) * c\n",
    "                output_real = model(text_real)\n",
    "                loss_real = criterion(output_real, lab_real)\n",
    "                gw_real = torch.autograd.grad(loss_real, model_parameters)\n",
    "                gw_real = [_.detach().clone() for _ in gw_real]\n",
    "                output_syn = model(text_syn)\n",
    "                loss_syn = criterion(output_syn, lab_syn)\n",
    "                gw_syn = torch.autograd.grad(loss_syn, model_parameters, create_graph=True)\n",
    "                loss += sum(torch.nn.functional.mse_loss(gw_real_, gw_syn_) for gw_real_, gw_syn_ in zip(gw_real, gw_syn))\n",
    "\n",
    "            optimizer_syn.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer_syn.step()\n",
    "            loss_avg += loss.item()\n",
    "\n",
    "            text_syn_train, label_syn_train = synthetic_text_data.detach(), synthetic_labels.detach()\n",
    "            dst_syn_train = DataLoader(torch.utils.data.TensorDataset(text_syn_train, label_syn_train), batch_size=batch_train, shuffle=True)\n",
    "\n",
    "            for il in tqdm(range(n_inner_loop)):\n",
    "                epoch_loss, epoch_acc = 0, 0\n",
    "                model.train()\n",
    "                for batch_text, batch_labels in dst_syn_train:\n",
    "                    batch_text, batch_labels = batch_text.to(device), batch_labels.to(device)\n",
    "                    optimizer_net.zero_grad()\n",
    "                    outputs = model(batch_text)\n",
    "                    loss = criterion(outputs, batch_labels)\n",
    "                    loss.backward()\n",
    "                    optimizer_net.step()\n",
    "                    epoch_loss += loss.item()\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    epoch_acc += torch.sum(preds == batch_labels).item()\n",
    "\n",
    "                epoch_loss /= len(dst_syn_train.dataset)\n",
    "                epoch_acc /= len(dst_syn_train.dataset)\n",
    "                print(f'Inner loop {il + 1}/{n_inner_loop}, Loss: {epoch_loss:.4f}, Accuracy: {epoch_acc:.4f}')\n",
    "\n",
    "        loss_avg /= (num_classes * n_outer_loop)\n",
    "        if it % 10 == 0:\n",
    "            print(f'Iter = {it:04d}, Loss = {loss_avg:.4f}')\n",
    "\n",
    "    data_save.append([synthetic_text_data.detach().cpu(), synthetic_labels.detach().cpu()])\n",
    "    torch.save({'data': data_save}, 'results_synthetic_data.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7acba803-a8e7-4338-826c-ffdf3e0645cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Word2Vec embeddings...\n",
      "Using device: cuda\n",
      "Loading AG_NEWS dataset...\n",
      "Building vocabulary...\n",
      "Creating embeddings matrix...\n",
      "\n",
      "================== Exp 0 ==================\n",
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "only Tensors of floating point and complex dtype can require gradients",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 207\u001b[0m\n\u001b[1;32m    204\u001b[0m synthetic_text_data \u001b[38;5;241m=\u001b[39m pad_sequences(synthetic_text_data)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    205\u001b[0m synthetic_labels \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(synthetic_labels, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m--> 207\u001b[0m \u001b[43msynthetic_text_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequires_grad\u001b[49m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    209\u001b[0m optimizer_syn \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mSGD([synthetic_text_data], lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m, momentum\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.9\u001b[39m)\n\u001b[1;32m    210\u001b[0m optimizer_syn\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "\u001b[0;31mRuntimeError\u001b[0m: only Tensors of floating point and complex dtype can require gradients"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import gensim.downloader as api\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "print(\"Loading Word2Vec embeddings...\")\n",
    "word2vec = api.load('word2vec-google-news-300')\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load AG_NEWS dataset using Hugging Face datasets library\n",
    "print(\"Loading AG_NEWS dataset...\")\n",
    "dataset = load_dataset('ag_news')\n",
    "\n",
    "# Tokenizer function\n",
    "def tokenize(text):\n",
    "    return text.lower().split()\n",
    "\n",
    "# Build vocabulary\n",
    "def build_vocab(texts, tokenizer):\n",
    "    vocab = {\"<unk>\": 0}\n",
    "    for text in texts:\n",
    "        for token in tokenizer(text):\n",
    "            if token not in vocab:\n",
    "                vocab[token] = len(vocab)\n",
    "    return vocab\n",
    "\n",
    "# Custom Dataset class\n",
    "class AGNewsDataset(Dataset):\n",
    "    def __init__(self, texts, labels, vocab, tokenizer):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.vocab = vocab\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        tokenized_text = [self.vocab[token] if token in self.vocab else self.vocab[\"<unk>\"]\n",
    "                          for token in self.tokenizer(text)]\n",
    "        return torch.tensor(tokenized_text, dtype=torch.long), torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "def pad_sequences(sequences, max_len=None, pad_value=0):\n",
    "    if not max_len:\n",
    "        max_len = max(len(seq) for seq in sequences)\n",
    "    padded_seqs = torch.full((len(sequences), max_len), pad_value, dtype=torch.long)\n",
    "    for i, seq in enumerate(sequences):\n",
    "        length = len(seq)\n",
    "        padded_seqs[i, :length] = seq\n",
    "    return padded_seqs\n",
    "\n",
    "# Extract texts and labels\n",
    "train_texts = [item['text'] for item in dataset['train']]\n",
    "train_labels = [item['label'] for item in dataset['train']]\n",
    "test_texts = [item['text'] for item in dataset['test']]\n",
    "test_labels = [item['label'] for item in dataset['test']]\n",
    "\n",
    "# Build vocabulary from training texts\n",
    "print(\"Building vocabulary...\")\n",
    "vocab = build_vocab(train_texts, tokenize)\n",
    "\n",
    "# Create Dataset and DataLoader\n",
    "train_dataset = AGNewsDataset(train_texts, train_labels, vocab, tokenize)\n",
    "test_dataset = AGNewsDataset(test_texts, test_labels, vocab, tokenize)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    texts, labels = zip(*batch)\n",
    "    texts_padded = pad_sequences(texts)\n",
    "    labels = torch.tensor(labels, dtype=torch.long)\n",
    "    return texts_padded, labels\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "# Create embedding matrix\n",
    "print(\"Creating embeddings matrix...\")\n",
    "embedding_dim = 300\n",
    "embedding_matrix = np.zeros((len(vocab), embedding_dim))\n",
    "for word, idx in vocab.items():\n",
    "    if word in word2vec:\n",
    "        embedding_matrix[idx] = word2vec[word]\n",
    "    else:\n",
    "        embedding_matrix[idx] = np.random.normal(scale=0.6, size=(embedding_dim,))\n",
    "\n",
    "embedding_matrix = torch.tensor(embedding_matrix, dtype=torch.float32).to(device)\n",
    "\n",
    "# Define the LSTM model\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, embedding_matrix, hidden_dim, output_dim, n_layers, drop_prob=0.5):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        num_embeddings, embedding_dim = embedding_matrix.shape\n",
    "        self.embedding = nn.Embedding.from_pretrained(embedding_matrix, freeze=True)  # Freeze embeddings\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=n_layers, dropout=drop_prob, batch_first=True)\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        lstm_out, (ht, ct) = self.lstm(x)\n",
    "        out = self.dropout(ht[-1])\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "hidden_dim = 256\n",
    "output_dim = 4  # AG_NEWS has 4 classes\n",
    "n_layers = 2\n",
    "model = LSTMClassifier(embedding_matrix, hidden_dim, output_dim, n_layers).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "def evaluate_syn(model, synthetic_text_data, synthetic_labels, test_loader, device, num_epochs=10, lr=0.001):\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer_net = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    synthetic_dataset = DataLoader(torch.utils.data.TensorDataset(synthetic_text_data.detach(), synthetic_labels), batch_size=32, shuffle=True)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for inputs, labels in tqdm(synthetic_dataset, desc=f'Training Epoch {epoch+1}/{num_epochs}', unit='batch'):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer_net.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer_net.step()\n",
    "            train_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "        train_accuracy = 100 * correct / total\n",
    "        if epoch % 9 == 0:\n",
    "            print(f'Epoch [{epoch + 1}/{num_epochs}], Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.2f}%')\n",
    "\n",
    "    model.eval()\n",
    "    total_acc, total_count = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for text, labels in tqdm(test_loader, desc='Evaluating on test set', unit='batch'):\n",
    "            text, labels = text.to(device), labels.to(device)\n",
    "            output = model(text)\n",
    "            _, predicted = torch.max(output, 1)\n",
    "            total_acc += (predicted == labels).sum().item()\n",
    "            total_count += labels.size(0)\n",
    "\n",
    "    overall_accuracy = total_acc / total_count\n",
    "    print(f'Test Accuracy: {overall_accuracy:.4f}')\n",
    "\n",
    "    results = {\n",
    "        'test_accuracy': overall_accuracy,\n",
    "    }\n",
    "    with open('results.pkl', 'wb') as f:\n",
    "        pickle.dump(results, f)\n",
    "\n",
    "    return overall_accuracy\n",
    "\n",
    "n_experiments = 1\n",
    "num_classes = len(set(train_labels))\n",
    "n_spc = 10  # samples_per_class\n",
    "n_iterations = 1000\n",
    "batch_train = 32\n",
    "batch_real = 32\n",
    "batch_syn = 32\n",
    "n_outer_loop = 1\n",
    "n_inner_loop = 1\n",
    "num_eval_epochs = 20\n",
    "\n",
    "data_save = []\n",
    "accs_hist = []\n",
    "\n",
    "def get_class_indices(labels, n_classes):\n",
    "    class_indices = [[] for _ in range(n_classes)]\n",
    "    for idx, label in enumerate(labels):\n",
    "        class_indices[label].append(idx)\n",
    "    return class_indices\n",
    "\n",
    "train_labels_list = [item['label'] for item in dataset['train']]\n",
    "class_indices = get_class_indices(train_labels_list, num_classes)\n",
    "\n",
    "for exp_n in range(n_experiments):\n",
    "    print(f'\\n================== Exp {exp_n} ==================\\n')\n",
    "\n",
    "    # Randomly select synthetic data from train dataset per class\n",
    "    synthetic_text_data = []\n",
    "    synthetic_labels = []\n",
    "    for class_idx in range(num_classes):\n",
    "        selected_indices = random.sample(class_indices[class_idx], n_spc)\n",
    "        for idx in selected_indices:\n",
    "            synthetic_text_data.append(train_dataset[idx][0])\n",
    "            synthetic_labels.append(train_dataset[idx][1])\n",
    "    synthetic_text_data = pad_sequences(synthetic_text_data).to(device)\n",
    "    synthetic_labels = torch.tensor(synthetic_labels, dtype=torch.long).to(device)\n",
    "    \n",
    "    synthetic_text_data.requires_grad = True\n",
    "\n",
    "    optimizer_syn = optim.SGD([synthetic_text_data], lr=0.01, momentum=0.9)\n",
    "    optimizer_syn.zero_grad()\n",
    "    criterion = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "    print('Training begins')\n",
    "\n",
    "    for it in range(n_iterations + 1):\n",
    "        model = LSTMClassifier(embedding_matrix, hidden_dim, output_dim, n_layers).to(device)\n",
    "        model.train()\n",
    "        model_parameters = list(model.parameters())\n",
    "        optimizer_net = optim.SGD(model.parameters(), lr=0.01)\n",
    "        optimizer_net.zero_grad()\n",
    "        loss_avg = 0\n",
    "\n",
    "        accs_hist.append(evaluate_syn(model, synthetic_text_data, synthetic_labels, test_loader, device, num_epochs=10, lr=0.001))\n",
    "\n",
    "        for ol in tqdm(range(n_outer_loop)):\n",
    "            print('Starting outer loop')\n",
    "            loss = torch.tensor(0.0).to(device)\n",
    "\n",
    "            for c in range(num_classes):\n",
    "                text_real, lab_real = next(iter(train_loader))\n",
    "                text_real, lab_real = text_real.to(device), lab_real.to(device)\n",
    "                lab_real = torch.ones((text_real.shape[0],), device=device, dtype=torch.long) * c\n",
    "                text_syn = synthetic_text_data[c * n_spc:(c + 1) * n_spc].to(device)\n",
    "                lab_syn = torch.ones((n_spc,), device=device, dtype=torch.long) * c\n",
    "                output_real = model(text_real)\n",
    "                loss_real = criterion(output_real, lab_real)\n",
    "                gw_real = torch.autograd.grad(loss_real, model_parameters, retain_graph=True)\n",
    "                gw_real = [_.detach().clone() for _ in gw_real]\n",
    "                output_syn = model(text_syn)\n",
    "                loss_syn = criterion(output_syn, lab_syn)\n",
    "                gw_syn = torch.autograd.grad(loss_syn, model_parameters, create_graph=True)\n",
    "                loss += sum(torch.nn.functional.mse_loss(gw_real_, gw_syn_) for gw_real_, gw_syn_ in zip(gw_real, gw_syn))\n",
    "\n",
    "            optimizer_syn.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer_syn.step()\n",
    "            loss_avg += loss.item()\n",
    "\n",
    "            text_syn_train, label_syn_train = synthetic_text_data.detach(), synthetic_labels.detach()\n",
    "            dst_syn_train = DataLoader(torch.utils.data.TensorDataset(text_syn_train, label_syn_train), batch_size=batch_train, shuffle=True)\n",
    "\n",
    "            for il in tqdm(range(n_inner_loop)):\n",
    "                epoch_loss, epoch_acc = 0, 0\n",
    "                model.train()\n",
    "                for batch_text, batch_labels in dst_syn_train:\n",
    "                    batch_text, batch_labels = batch_text.to(device), batch_labels.to(device)\n",
    "                    optimizer_net.zero_grad()\n",
    "                    outputs = model(batch_text)\n",
    "                    loss = criterion(outputs, batch_labels)\n",
    "                    loss.backward()\n",
    "                    optimizer_net.step()\n",
    "                    epoch_loss += loss.item()\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    epoch_acc += torch.sum(preds == batch_labels).item()\n",
    "\n",
    "                epoch_loss /= len(dst_syn_train.dataset)\n",
    "                epoch_acc /= len(dst_syn_train.dataset)\n",
    "                print(f'Inner loop {il + 1}/{n_inner_loop}, Loss: {epoch_loss:.4f}, Accuracy: {epoch_acc:.4f}')\n",
    "\n",
    "        loss_avg /= (num_classes * n_outer_loop)\n",
    "        if it % 10 == 0:\n",
    "            print(f'Iter = {it:04d}, Loss = {loss_avg:.4f}')\n",
    "\n",
    "    data_save.append([synthetic_text_data.detach().cpu(), synthetic_labels.detach().cpu()])\n",
    "    torch.save({'data': data_save}, 'results_synthetic_data.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "81fea663-1a91-46a8-a6da-8ee4241a36e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Word2Vec embeddings...\n",
      "Using device: cuda\n",
      "Loading AG_NEWS dataset...\n",
      "Building vocabulary...\n",
      "Creating embeddings matrix...\n",
      "\n",
      "================== Exp 0 ==================\n",
      "\n",
      "Training begins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1/10:   0%|                                                                                                                         | 0/2 [00:00<?, ?batch/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got torch.cuda.FloatTensor instead (while checking arguments for embedding)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 234\u001b[0m\n\u001b[1;32m    231\u001b[0m optimizer_net\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m    232\u001b[0m loss_avg \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m--> 234\u001b[0m accs_hist\u001b[38;5;241m.\u001b[39mappend(\u001b[43mevaluate_syn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msynthetic_data_embeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msynthetic_text_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msynthetic_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.001\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    236\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ol \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(n_outer_loop)):\n\u001b[1;32m    237\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mStarting outer loop\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[11], line 147\u001b[0m, in \u001b[0;36mevaluate_syn\u001b[0;34m(model, synthetic_data_embeddings, synthetic_text_data, synthetic_labels, test_loader, device, num_epochs, lr)\u001b[0m\n\u001b[1;32m    145\u001b[0m inputs, labels \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mto(device), labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    146\u001b[0m optimizer_net\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m--> 147\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    148\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[1;32m    149\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/anaconda3/envs/dataset_distill/lib/python3.9/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/dataset_distill/lib/python3.9/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[11], line 108\u001b[0m, in \u001b[0;36mLSTMClassifier.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m--> 108\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    109\u001b[0m     lstm_out, (ht, ct) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlstm(x)\n\u001b[1;32m    110\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(ht[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n",
      "File \u001b[0;32m~/anaconda3/envs/dataset_distill/lib/python3.9/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/dataset_distill/lib/python3.9/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/dataset_distill/lib/python3.9/site-packages/torch/nn/modules/sparse.py:164\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 164\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/dataset_distill/lib/python3.9/site-packages/torch/nn/functional.py:2267\u001b[0m, in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2261\u001b[0m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[1;32m   2262\u001b[0m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[1;32m   2263\u001b[0m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[1;32m   2264\u001b[0m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[1;32m   2265\u001b[0m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[1;32m   2266\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[0;32m-> 2267\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got torch.cuda.FloatTensor instead (while checking arguments for embedding)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import gensim.downloader as api\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "print(\"Loading Word2Vec embeddings...\")\n",
    "word2vec = api.load('word2vec-google-news-300')\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load AG_NEWS dataset using Hugging Face datasets library\n",
    "print(\"Loading AG_NEWS dataset...\")\n",
    "dataset = load_dataset('ag_news')\n",
    "\n",
    "# Tokenizer function\n",
    "def tokenize(text):\n",
    "    return text.lower().split()\n",
    "\n",
    "# Build vocabulary\n",
    "def build_vocab(texts, tokenizer):\n",
    "    vocab = {\"<unk>\": 0}\n",
    "    for text in texts:\n",
    "        for token in tokenizer(text):\n",
    "            if token not in vocab:\n",
    "                vocab[token] = len(vocab)\n",
    "    return vocab\n",
    "\n",
    "# Custom Dataset class\n",
    "class AGNewsDataset(Dataset):\n",
    "    def __init__(self, texts, labels, vocab, tokenizer):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.vocab = vocab\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        tokenized_text = [self.vocab[token] if token in self.vocab else self.vocab[\"<unk>\"]\n",
    "                          for token in self.tokenizer(text)]\n",
    "        return torch.tensor(tokenized_text, dtype=torch.long), torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "def pad_sequences(sequences, max_len=None, pad_value=0):\n",
    "    if not max_len:\n",
    "        max_len = max(len(seq) for seq in sequences)\n",
    "    padded_seqs = torch.full((len(sequences), max_len), pad_value, dtype=torch.long)\n",
    "    for i, seq in enumerate(sequences):\n",
    "        length = len(seq)\n",
    "        padded_seqs[i, :length] = seq\n",
    "    return padded_seqs\n",
    "\n",
    "# Extract texts and labels\n",
    "train_texts = [item['text'] for item in dataset['train']]\n",
    "train_labels = [item['label'] for item in dataset['train']]\n",
    "test_texts = [item['text'] for item in dataset['test']]\n",
    "test_labels = [item['label'] for item in dataset['test']]\n",
    "\n",
    "# Build vocabulary from training texts\n",
    "print(\"Building vocabulary...\")\n",
    "vocab = build_vocab(train_texts, tokenize)\n",
    "\n",
    "# Create Dataset and DataLoader\n",
    "train_dataset = AGNewsDataset(train_texts, train_labels, vocab, tokenize)\n",
    "test_dataset = AGNewsDataset(test_texts, test_labels, vocab, tokenize)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    texts, labels = zip(*batch)\n",
    "    texts_padded = pad_sequences(texts)\n",
    "    labels = torch.tensor(labels, dtype=torch.long)\n",
    "    return texts_padded, labels\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "# Create embedding matrix\n",
    "print(\"Creating embeddings matrix...\")\n",
    "embedding_dim = 300\n",
    "embedding_matrix = np.zeros((len(vocab), embedding_dim))\n",
    "for word, idx in vocab.items():\n",
    "    if word in word2vec:\n",
    "        embedding_matrix[idx] = word2vec[word]\n",
    "    else:\n",
    "        embedding_matrix[idx] = np.random.normal(scale=0.6, size=(embedding_dim,))\n",
    "\n",
    "embedding_matrix = torch.tensor(embedding_matrix, dtype=torch.float32).to(device)\n",
    "\n",
    "# Define the LSTM model\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, embedding_matrix, hidden_dim, output_dim, n_layers, drop_prob=0.5):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        num_embeddings, embedding_dim = embedding_matrix.shape\n",
    "        self.embedding = nn.Embedding.from_pretrained(embedding_matrix, freeze=True)  # Freeze embeddings\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=n_layers, dropout=drop_prob, batch_first=True)\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        lstm_out, (ht, ct) = self.lstm(x)\n",
    "        out = self.dropout(ht[-1])\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "hidden_dim = 256\n",
    "output_dim = 4  # AG_NEWS has 4 classes\n",
    "n_layers = 2\n",
    "model = LSTMClassifier(embedding_matrix, hidden_dim, output_dim, n_layers).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "class SyntheticDataEmbeddings(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, num_samples):\n",
    "        super(SyntheticDataEmbeddings, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.embeddings.weight.data.normal_(0, 1)\n",
    "        self.embeddings.weight.requires_grad = True\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.embeddings(x)\n",
    "\n",
    "def evaluate_syn(model, synthetic_data_embeddings, synthetic_text_data, synthetic_labels, test_loader, device, num_epochs=10, lr=0.001):\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer_net = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    synthetic_text_data = synthetic_data_embeddings(synthetic_text_data)\n",
    "    synthetic_dataset = DataLoader(torch.utils.data.TensorDataset(synthetic_text_data.detach(), synthetic_labels), batch_size=32, shuffle=True)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for inputs, labels in tqdm(synthetic_dataset, desc=f'Training Epoch {epoch+1}/{num_epochs}', unit='batch'):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer_net.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer_net.step()\n",
    "            train_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "        train_accuracy = 100 * correct / total\n",
    "        if epoch % 9 == 0:\n",
    "            print(f'Epoch [{epoch + 1}/{num_epochs}], Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.2f}%')\n",
    "\n",
    "    model.eval()\n",
    "    total_acc, total_count = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for text, labels in tqdm(test_loader, desc='Evaluating on test set', unit='batch'):\n",
    "            text, labels = text.to(device), labels.to(device)\n",
    "            output = model(text)\n",
    "            _, predicted = torch.max(output, 1)\n",
    "            total_acc += (predicted == labels).sum().item()\n",
    "            total_count += labels.size(0)\n",
    "\n",
    "    overall_accuracy = total_acc / total_count\n",
    "    print(f'Test Accuracy: {overall_accuracy:.4f}')\n",
    "\n",
    "    results = {\n",
    "        'test_accuracy': overall_accuracy,\n",
    "    }\n",
    "    with open('results.pkl', 'wb') as f:\n",
    "        pickle.dump(results, f)\n",
    "\n",
    "    return overall_accuracy\n",
    "\n",
    "n_experiments = 1\n",
    "num_classes = len(set(train_labels))\n",
    "n_spc = 10  # samples_per_class\n",
    "n_iterations = 1000\n",
    "batch_train = 32\n",
    "batch_real = 32\n",
    "batch_syn = 32\n",
    "n_outer_loop = 1\n",
    "n_inner_loop = 1\n",
    "num_eval_epochs = 20\n",
    "\n",
    "data_save = []\n",
    "accs_hist = []\n",
    "\n",
    "def get_class_indices(labels, n_classes):\n",
    "    class_indices = [[] for _ in range(n_classes)]\n",
    "    for idx, label in enumerate(labels):\n",
    "        class_indices[label].append(idx)\n",
    "    return class_indices\n",
    "\n",
    "train_labels_list = [item['label'] for item in dataset['train']]\n",
    "class_indices = get_class_indices(train_labels_list, num_classes)\n",
    "\n",
    "for exp_n in range(n_experiments):\n",
    "    print(f'\\n================== Exp {exp_n} ==================\\n')\n",
    "\n",
    "    # Randomly select synthetic data from train dataset per class\n",
    "    synthetic_text_data = []\n",
    "    synthetic_labels = []\n",
    "    for class_idx in range(num_classes):\n",
    "        selected_indices = random.sample(class_indices[class_idx], n_spc)\n",
    "        for idx in selected_indices:\n",
    "            synthetic_text_data.append(train_dataset[idx][0])\n",
    "            synthetic_labels.append(train_dataset[idx][1])\n",
    "    synthetic_text_data = pad_sequences(synthetic_text_data).to(device)\n",
    "    synthetic_labels = torch.tensor(synthetic_labels, dtype=torch.long).to(device)\n",
    "    \n",
    "    synthetic_data_embeddings = SyntheticDataEmbeddings(len(vocab), embedding_dim, num_classes * n_spc).to(device)\n",
    "\n",
    "    optimizer_syn = optim.SGD(synthetic_data_embeddings.parameters(), lr=0.01, momentum=0.9)\n",
    "    optimizer_syn.zero_grad()\n",
    "    criterion = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "    print('Training begins')\n",
    "\n",
    "    for it in range(n_iterations + 1):\n",
    "        model = LSTMClassifier(embedding_matrix, hidden_dim, output_dim, n_layers).to(device)\n",
    "        model.train()\n",
    "        model_parameters = list(model.parameters())\n",
    "        optimizer_net = optim.SGD(model.parameters(), lr=0.01)\n",
    "        optimizer_net.zero_grad()\n",
    "        loss_avg = 0\n",
    "\n",
    "        accs_hist.append(evaluate_syn(model, synthetic_data_embeddings, synthetic_text_data, synthetic_labels, test_loader, device, num_epochs=10, lr=0.001))\n",
    "\n",
    "        for ol in tqdm(range(n_outer_loop)):\n",
    "            print('Starting outer loop')\n",
    "            loss = torch.tensor(0.0).to(device)\n",
    "\n",
    "            for c in range(num_classes):\n",
    "                text_real, lab_real = next(iter(train_loader))\n",
    "                text_real, lab_real = text_real.to(device), lab_real.to(device)\n",
    "                lab_real = torch.ones((text_real.shape[0],), device=device, dtype=torch.long) * c\n",
    "                text_syn = synthetic_text_data[c * n_spc:(c + 1) * n_spc].to(device)\n",
    "                text_syn = synthetic_data_embeddings(text_syn)\n",
    "                lab_syn = torch.ones((n_spc,), device=device, dtype=torch.long) * c\n",
    "                output_real = model(text_real)\n",
    "                loss_real = criterion(output_real, lab_real)\n",
    "                gw_real = torch.autograd.grad(loss_real, model_parameters, retain_graph=True)\n",
    "                gw_real = [_.detach().clone() for _ in gw_real]\n",
    "                output_syn = model(text_syn)\n",
    "                loss_syn = criterion(output_syn, lab_syn)\n",
    "                gw_syn = torch.autograd.grad(loss_syn, model_parameters, create_graph=True)\n",
    "                loss += sum(torch.nn.functional.mse_loss(gw_real_, gw_syn_) for gw_real_, gw_syn_ in zip(gw_real, gw_syn))\n",
    "\n",
    "            optimizer_syn.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer_syn.step()\n",
    "            loss_avg += loss.item()\n",
    "\n",
    "            text_syn_train, label_syn_train = synthetic_text_data.detach(), synthetic_labels.detach()\n",
    "            dst_syn_train = DataLoader(torch.utils.data.TensorDataset(text_syn_train, label_syn_train), batch_size=batch_train, shuffle=True)\n",
    "\n",
    "            for il in tqdm(range(n_inner_loop)):\n",
    "                epoch_loss, epoch_acc = 0, 0\n",
    "                model.train()\n",
    "                for batch_text, batch_labels in dst_syn_train:\n",
    "                    batch_text, batch_labels = batch_text.to(device), batch_labels.to(device)\n",
    "                    optimizer_net.zero_grad()\n",
    "                    outputs = model(batch_text)\n",
    "                    loss = criterion(outputs, batch_labels)\n",
    "                    loss.backward()\n",
    "                    optimizer_net.step()\n",
    "                    epoch_loss += loss.item()\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    epoch_acc += torch.sum(preds == batch_labels).item()\n",
    "\n",
    "                epoch_loss /= len(dst_syn_train.dataset)\n",
    "                epoch_acc /= len(dst_syn_train.dataset)\n",
    "                print(f'Inner loop {il + 1}/{n_inner_loop}, Loss: {epoch_loss:.4f}, Accuracy: {epoch_acc:.4f}')\n",
    "\n",
    "        loss_avg /= (num_classes * n_outer_loop)\n",
    "        if it % 10 == 0:\n",
    "            print(f'Iter = {it:04d}, Loss = {loss_avg:.4f}')\n",
    "\n",
    "    data_save.append([synthetic_text_data.detach().cpu(), synthetic_labels.detach().cpu()])\n",
    "    torch.save({'data': data_save}, 'results_synthetic_data.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeafaebc-de6e-477e-b6e2-e8d04c45a80d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
