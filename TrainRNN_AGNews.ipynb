{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8628a3a7-44e9-4c46-b418-ab2083e0877f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /home/IAIS/rrao/.local/lib/python3.9/site-packages (2.14.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/IAIS/rrao/.local/lib/python3.9/site-packages (from datasets) (1.25.1)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /home/IAIS/rrao/.local/lib/python3.9/site-packages (from datasets) (12.0.1)\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /home/IAIS/rrao/.local/lib/python3.9/site-packages (from datasets) (0.3.7)\n",
      "Requirement already satisfied: pandas in /home/IAIS/rrao/.local/lib/python3.9/site-packages (from datasets) (2.0.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in /home/IAIS/rrao/anaconda3/envs/dataset_distill/lib/python3.9/site-packages (from datasets) (2.32.2)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /home/IAIS/rrao/.local/lib/python3.9/site-packages (from datasets) (4.65.0)\n",
      "Requirement already satisfied: xxhash in /home/IAIS/rrao/.local/lib/python3.9/site-packages (from datasets) (3.2.0)\n",
      "Requirement already satisfied: multiprocess in /home/IAIS/rrao/.local/lib/python3.9/site-packages (from datasets) (0.70.15)\n",
      "Requirement already satisfied: fsspec>=2021.11.1 in /home/IAIS/rrao/.local/lib/python3.9/site-packages (from fsspec[http]>=2021.11.1->datasets) (2023.6.0)\n",
      "Requirement already satisfied: aiohttp in /home/IAIS/rrao/.local/lib/python3.9/site-packages (from datasets) (3.8.5)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in /home/IAIS/rrao/.local/lib/python3.9/site-packages (from datasets) (0.16.4)\n",
      "Requirement already satisfied: packaging in /home/IAIS/rrao/.local/lib/python3.9/site-packages (from datasets) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/IAIS/rrao/.local/lib/python3.9/site-packages (from datasets) (6.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/IAIS/rrao/.local/lib/python3.9/site-packages (from aiohttp->datasets) (23.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /home/IAIS/rrao/.local/lib/python3.9/site-packages (from aiohttp->datasets) (3.3.2)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/IAIS/rrao/.local/lib/python3.9/site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /home/IAIS/rrao/.local/lib/python3.9/site-packages (from aiohttp->datasets) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/IAIS/rrao/.local/lib/python3.9/site-packages (from aiohttp->datasets) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/IAIS/rrao/.local/lib/python3.9/site-packages (from aiohttp->datasets) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/IAIS/rrao/.local/lib/python3.9/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: filelock in /home/IAIS/rrao/.local/lib/python3.9/site-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (3.12.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/IAIS/rrao/.local/lib/python3.9/site-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (4.7.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/IAIS/rrao/anaconda3/envs/dataset_distill/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/IAIS/rrao/anaconda3/envs/dataset_distill/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/IAIS/rrao/anaconda3/envs/dataset_distill/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (2024.6.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/IAIS/rrao/.local/lib/python3.9/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/IAIS/rrao/.local/lib/python3.9/site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /home/IAIS/rrao/.local/lib/python3.9/site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in /home/IAIS/rrao/.local/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1da4fa11-985b-429c-a51e-1f33c7225cda",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/IAIS/rrao/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Download NLTK data\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Load the AG News dataset\n",
    "dataset = load_dataset(\"ag_news\")\n",
    "\n",
    "# NLTK Tokenizer Function\n",
    "def nltk_tokenizer(text):\n",
    "    return word_tokenize(text)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "class AGNewsDataset(Dataset):\n",
    "    def __init__(self, texts, labels, vocab, max_length):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.vocab = vocab\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        tokenized_text = [self.vocab.get(token, self.vocab['<UNK>']) for token in text]\n",
    "        if len(tokenized_text) < self.max_length:\n",
    "            tokenized_text += [self.vocab['<PAD>']] * (self.max_length - len(tokenized_text))\n",
    "        else:\n",
    "            tokenized_text = tokenized_text[:self.max_length]\n",
    "        return torch.tensor(tokenized_text, dtype=torch.long), torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "# Build vocabulary\n",
    "def build_vocab(dataset, tokenizer):\n",
    "    vocab = {'<PAD>': 0, '<UNK>': 1}\n",
    "    for example in dataset:\n",
    "        tokens = tokenizer(example['text'])\n",
    "        for token in tokens:\n",
    "            if token not in vocab:\n",
    "                vocab[token] = len(vocab)\n",
    "    return vocab\n",
    "\n",
    "# Tokenize and build vocab\n",
    "train_texts = [nltk_tokenizer(example['text']) for example in dataset['train']]\n",
    "train_labels = [example['label'] for example in dataset['train']]\n",
    "vocab = build_vocab(dataset['train'], nltk_tokenizer)\n",
    "\n",
    "# Set max length for padding\n",
    "max_length = 128\n",
    "\n",
    "# Create dataset\n",
    "full_dataset = AGNewsDataset(train_texts, train_labels, vocab, max_length)\n",
    "\n",
    "# Split training set into training and validation sets\n",
    "train_size = int(0.8 * len(full_dataset))\n",
    "val_size = len(full_dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])\n",
    "\n",
    "# Prepare test dataset\n",
    "test_texts = [nltk_tokenizer(example['text']) for example in dataset['test']]\n",
    "test_labels = [example['label'] for example in dataset['test']]\n",
    "test_dataset = AGNewsDataset(test_texts, test_labels, vocab, max_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42c0836e-9e41-45b1-8ae0-10a0b2c26b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, output_dim):\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "        self.rnn = nn.RNN(embed_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        output, hidden = self.rnn(x)\n",
    "        return self.fc(hidden[-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f6647377-6bb4-4d17-84d4-6d39be0d671f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Parameters\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 10\n",
    "EMBED_DIM = 128\n",
    "HIDDEN_DIM = 256\n",
    "OUTPUT_DIM = 4\n",
    "LR = 0.001\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# DataLoader\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# Model, loss function, and optimizer\n",
    "model = RNNModel(len(vocab), EMBED_DIM, HIDDEN_DIM, OUTPUT_DIM).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e71400d5-fbcd-4721-9b0e-894f971fad84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Validation Accuracy: 0.2477\n",
      "Epoch 2/10, Validation Accuracy: 0.2405\n",
      "Epoch 3/10, Validation Accuracy: 0.2455\n",
      "Epoch 4/10, Validation Accuracy: 0.2500\n",
      "Epoch 5/10, Validation Accuracy: 0.2359\n",
      "Epoch 6/10, Validation Accuracy: 0.2547\n",
      "Epoch 7/10, Validation Accuracy: 0.2642\n",
      "Epoch 8/10, Validation Accuracy: 0.2630\n",
      "Epoch 9/10, Validation Accuracy: 0.2533\n",
      "Epoch 10/10, Validation Accuracy: 0.2387\n",
      "Test Accuracy: 0.2330\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       World       0.00      0.00      0.00      1900\n",
      "      Sports       0.24      0.49      0.32      1900\n",
      "    Business       0.00      0.00      0.00      1900\n",
      "    Sci/Tech       0.22      0.45      0.30      1900\n",
      "\n",
      "    accuracy                           0.23      7600\n",
      "   macro avg       0.12      0.23      0.16      7600\n",
      "weighted avg       0.12      0.23      0.16      7600\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/IAIS/rrao/.local/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/IAIS/rrao/.local/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/IAIS/rrao/.local/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    for texts, labels in train_loader:\n",
    "        texts, labels = texts.to(device), labels.to(device)\n",
    "        \n",
    "        outputs = model(texts)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    model.eval()\n",
    "    val_labels = []\n",
    "    val_preds = []\n",
    "    with torch.no_grad():\n",
    "        for texts, labels in val_loader:\n",
    "            texts, labels = texts.to(device), labels.to(device)\n",
    "            outputs = model(texts)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            val_labels.extend(labels.cpu().numpy())\n",
    "            val_preds.extend(preds.cpu().numpy())\n",
    "    \n",
    "    val_accuracy = accuracy_score(val_labels, val_preds)\n",
    "    print(f'Epoch {epoch + 1}/{EPOCHS}, Validation Accuracy: {val_accuracy:.4f}')\n",
    "\n",
    "# Final evaluation on test set\n",
    "model.eval()\n",
    "test_labels = []\n",
    "test_preds = []\n",
    "with torch.no_grad():\n",
    "    for texts, labels in test_loader:\n",
    "        texts, labels = texts.to(device), labels.to(device)\n",
    "        outputs = model(texts)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        test_labels.extend(labels.cpu().numpy())\n",
    "        test_preds.extend(preds.cpu().numpy())\n",
    "\n",
    "overall_accuracy = accuracy_score(test_labels, test_preds)\n",
    "class_report = classification_report(test_labels, test_preds, target_names=['World', 'Sports', 'Business', 'Sci/Tech'])\n",
    "\n",
    "print(f'Test Accuracy: {overall_accuracy:.4f}')\n",
    "print('Classification Report:')\n",
    "print(class_report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14aee1b6-a574-4761-9046-21dc188d26b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
