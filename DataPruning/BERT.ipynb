{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00ad6dd3-e1a2-4d4f-8878-588a95e348f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "from tqdm import tqdm\n",
    "\n",
    "def calculate_perplexity_with_bert(texts, tokenizer, model, device='cpu'):\n",
    "    \"\"\"\n",
    "    Calculate the perplexity for a list of texts using BERT in a masked language modeling fashion.\n",
    "    \n",
    "    Args:\n",
    "    texts (list of str): The texts to calculate perplexity for.\n",
    "    tokenizer: The tokenizer compatible with the model.\n",
    "    model: The language model (e.g., BERT).\n",
    "    device (str): The device to run the calculations on ('cpu' or 'cuda').\n",
    "\n",
    "    Returns:\n",
    "    list of float: The perplexity values for each text.\n",
    "    \"\"\"\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    model.to(device)  # Ensure the model is on the correct device\n",
    "    perplexities = []\n",
    "\n",
    "    for text in tqdm(texts, desc=\"Calculating Perplexity\"):\n",
    "        inputs = tokenizer(text, return_tensors='pt', max_length=128, truncation=True, padding='max_length').to(device)\n",
    "        \n",
    "        # Create labels by copying input_ids and masking a random word\n",
    "        labels = inputs['input_ids'].clone()\n",
    "        # Randomly mask one token in the input (other strategies can also be used)\n",
    "        rand_index = torch.randint(0, inputs['input_ids'].size(1), (1,))\n",
    "        inputs['input_ids'][0, rand_index] = tokenizer.mask_token_id\n",
    "\n",
    "        with torch.no_grad():  # Disable gradient calculation\n",
    "            outputs = model(**inputs, labels=labels)\n",
    "            \n",
    "            \n",
    "            loss = outputs.loss\n",
    "            perplexity = torch.exp(loss).item()  # Calculate perplexity\n",
    "            perplexities.append(perplexity)\n",
    "    \n",
    "    return perplexities\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ae27bcd3-362f-4b58-9408-03f1d3d713f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'\n",
    "def calculate_perplexity(texts, tokenizer, model):\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    perplexities = []\n",
    "    for text in tqdm(texts, desc=\"Calculating Perplexity\"):\n",
    "        inputs = tokenizer(text, return_tensors='pt', max_length=128, truncation=True, padding='max_length').to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs, labels=inputs['input_ids'])\n",
    "            loss = outputs.loss\n",
    "            perplexity = torch.exp(loss).item()\n",
    "            perplexities.append(perplexity)\n",
    "    return perplexities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8e9d5f10-bbf5-4530-8843-baf8fded906a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Calculating Perplexity: 100%|█████████████████████| 2/2 [00:00<00:00, 14.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[172285008.0, 24890076.0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForMaskedLM.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Example texts\n",
    "texts = [\"Hello, how are you?\", \"The quick brown fox jumps over the lazy dog.\"]\n",
    "\n",
    "# Calculate perplexities\n",
    "perplexities = calculate_perplexity(texts, tokenizer, model)\n",
    "\n",
    "print(perplexities)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e95288a2-ccfe-4611-bbdd-e8a5a1219a9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating Perplexity: 100%|█████████████████████| 2/2 [00:00<00:00, 29.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 101, 7592, 1010, 2129, 2024, 2017, 1029,  102,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0]], device='cuda:0')\n",
      "MaskedLMOutput(loss=tensor(18.9647, device='cuda:0'), logits=tensor([[[ -8.1436,  -8.0535,  -8.0647,  ...,  -7.0366,  -7.3415,  -4.6856],\n",
      "         [ -9.2450,  -9.1295,  -9.2528,  ...,  -8.2211,  -8.3913,  -5.7961],\n",
      "         [-15.6711, -16.1384, -16.2145,  ..., -13.1981, -12.7352, -11.4977],\n",
      "         ...,\n",
      "         [ -7.2814,  -7.5417,  -7.4203,  ...,  -6.7853,  -6.6089,  -4.3045],\n",
      "         [ -8.2528,  -8.5221,  -8.2929,  ...,  -7.7153,  -7.3839,  -5.0751],\n",
      "         [ -8.2223,  -8.5782,  -8.3146,  ...,  -7.6184,  -6.8882,  -5.5199]]],\n",
      "       device='cuda:0'), hidden_states=None, attentions=None)\n",
      "tensor([[  101,  1996,  4248,  2829,  4419, 14523,  2058,  1996, 13971,  3899,\n",
      "          1012,   102,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0]],\n",
      "       device='cuda:0')\n",
      "MaskedLMOutput(loss=tensor(17.0300, device='cuda:0'), logits=tensor([[[ -8.0251,  -8.0380,  -7.9877,  ...,  -7.2566,  -6.9989,  -5.6419],\n",
      "         [-16.6022, -16.6210, -16.5659,  ..., -14.5095, -13.6250, -14.4450],\n",
      "         [-10.7943, -11.5395, -11.1698,  ...,  -9.2282,  -8.9318, -10.7872],\n",
      "         ...,\n",
      "         [ -9.3496,  -9.3202,  -9.4906,  ...,  -9.2667,  -9.7959,  -7.9259],\n",
      "         [ -8.8074,  -8.8499,  -8.7827,  ...,  -8.6837,  -8.1463,  -9.7856],\n",
      "         [ -9.0133,  -9.0818,  -9.0415,  ...,  -8.9727,  -8.2689,  -9.2424]]],\n",
      "       device='cuda:0'), hidden_states=None, attentions=None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "model.to(device)\n",
    "perplexities = []\n",
    "for text in tqdm(texts, desc=\"Calculating Perplexity\"):\n",
    "    inputs = tokenizer(text, return_tensors='pt', max_length=128, truncation=True, padding='max_length').to(device)\n",
    "    print(inputs['input_ids'])\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, labels=inputs['input_ids'])\n",
    "        print(outputs)\n",
    "        loss = outputs.loss\n",
    "        perplexity = torch.exp(loss).item()\n",
    "        perplexities.append(perplexity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dc03ce26-a44e-4c91-9804-d77f3f2c698e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[172285008.0, 24890076.0]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perplexities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bbe7a20d-f4e6-46e4-afd1-3b8607641b2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  101,  1996,  4248,  2829,  4419, 14523,  2058,  1996, 13971,  3899,\n",
       "          1012,   102,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9c82af46-57d5-4b88-bc8d-bfc764b455f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:0')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs['attention_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d28137d8-2a41-4d89-8cf9-fa32b7568362",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model(**inputs, labels=inputs['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5a9341ee-4520-4985-bfa1-5d4fd80eeac9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(17.0300, device='cuda:0', grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs['loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8e1084e3-33c0-4a70-806d-3c91cf1c34d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24890076.0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.exp(loss).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d4ca36-e2c0-4adb-befb-1dfc82bfdc00",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
