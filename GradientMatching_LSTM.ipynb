{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d4d48a49-5bd9-4064-9ee4-2b0c78fb49a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/IAIS/rrao/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset, random_split, TensorDataset\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from datasets import load_dataset\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from gensim.models import KeyedVectors\n",
    "import copy\n",
    "import gensim.downloader as api\n",
    "\n",
    "# Load the Word2Vec model\n",
    "word2vec_model = api.load('word2vec-google-news-300')\n",
    "\n",
    "# Download NLTK data\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Load the AG News dataset\n",
    "dataset = load_dataset(\"ag_news\")\n",
    "\n",
    "# NLTK Tokenizer Function\n",
    "def nltk_tokenizer(text):\n",
    "    return word_tokenize(text.lower())\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "class AGNewsDataset(Dataset):\n",
    "    def __init__(self, texts, labels, vocab, max_length):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.vocab = vocab\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        tokenized_text = [self.vocab.get(token, self.vocab['<UNK>']) for token in text]\n",
    "        if len(tokenized_text) < self.max_length:\n",
    "            tokenized_text += [self.vocab['<PAD>']] * (self.max_length - len(tokenized_text))\n",
    "        else:\n",
    "            tokenized_text = tokenized_text[:self.max_length]\n",
    "        return torch.tensor(tokenized_text, dtype=torch.long), torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "# Build vocabulary\n",
    "def build_vocab(dataset, tokenizer):\n",
    "    vocab = {'<PAD>': 0, '<UNK>': 1}\n",
    "    for example in dataset:\n",
    "        tokens = tokenizer(example['text'])\n",
    "        for token in tokens:\n",
    "            if token not in vocab:\n",
    "                vocab[token] = len(vocab)\n",
    "    return vocab\n",
    "\n",
    "# Tokenize and build vocab\n",
    "train_texts = [nltk_tokenizer(example['text']) for example in dataset['train']]\n",
    "train_labels = [example['label'] for example in dataset['train']]\n",
    "vocab = build_vocab(dataset['train'], nltk_tokenizer)\n",
    "\n",
    "# Set max length for padding\n",
    "max_length = 128\n",
    "\n",
    "# Create dataset\n",
    "full_dataset = AGNewsDataset(train_texts, train_labels, vocab, max_length)\n",
    "\n",
    "# Split training set into training and validation sets\n",
    "train_size = int(0.8 * len(full_dataset))\n",
    "val_size = len(full_dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])\n",
    "\n",
    "# Prepare test dataset\n",
    "test_texts = [nltk_tokenizer(example['text']) for example in dataset['test']]\n",
    "test_labels = [example['label'] for example in dataset['test']]\n",
    "test_dataset = AGNewsDataset(test_texts, test_labels, vocab, max_length)\n",
    "\n",
    "# Initialize embedding matrix\n",
    "def build_embedding_matrix(vocab, word2vec_model, embedding_dim):\n",
    "    embedding_matrix = np.zeros((len(vocab), embedding_dim))\n",
    "    for word, idx in vocab.items():\n",
    "        if word in word2vec_model:\n",
    "            embedding_matrix[idx] = word2vec_model[word]\n",
    "        else:\n",
    "            embedding_matrix[idx] = np.random.normal(size=(embedding_dim,))\n",
    "    return torch.tensor(embedding_matrix, dtype=torch.float32)\n",
    "\n",
    "# Build the embedding matrix\n",
    "embedding_dim = 300  # Word2Vec uses 300-dimensional vectors\n",
    "embedding_matrix = build_embedding_matrix(vocab, word2vec_model, embedding_dim)\n",
    "\n",
    "class DeepLSTMModel(nn.Module):\n",
    "    def __init__(self, vocab_size, output_dim, embedding_matrix):\n",
    "        super(DeepLSTMModel, self).__init__()\n",
    "        self.embedding = nn.Embedding.from_pretrained(embedding_matrix, freeze=False)\n",
    "        self.lstm = nn.LSTM(embedding_matrix.size(1), 256, num_layers=3, batch_first=True, dropout=0.5)\n",
    "        self.fc = nn.Linear(256, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        output, (hidden, cell) = self.lstm(x)\n",
    "        return self.fc(hidden[-1])\n",
    "\n",
    "# Training Parameters\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 3\n",
    "OUTPUT_DIM = 4\n",
    "LR = 0.001\n",
    "num_iterations = 100\n",
    "\n",
    "\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# DataLoader\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# Model, loss function, and optimizer\n",
    "model = DeepLSTMModel(len(vocab), OUTPUT_DIM, embedding_matrix).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer_net = optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "# Synthetic data initialization\n",
    "num_classes = 4\n",
    "num_synthetic_per_class = 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "10164f83-dffd-45cd-98f9-1f9ffed964bc",
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "the derivative for '_cudnn_rnn_backward' is not implemented. Double backwards is not supported for CuDNN RNNs due to limitations in the CuDNN API. To run double backwards, please disable the CuDNN backend temporarily while running the forward pass of your RNN. For example: \nwith torch.backends.cudnn.flags(enabled=False):\n    output = model(inputs)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 39\u001b[0m\n\u001b[1;32m     37\u001b[0m loss_match \u001b[38;5;241m=\u001b[39m layerwise_matching_loss(gradients_synthetic, gradients_real)\n\u001b[1;32m     38\u001b[0m optimizer_syn\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 39\u001b[0m \u001b[43mloss_match\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m optimizer_syn\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     41\u001b[0m loss_avg \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss_match\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: the derivative for '_cudnn_rnn_backward' is not implemented. Double backwards is not supported for CuDNN RNNs due to limitations in the CuDNN API. To run double backwards, please disable the CuDNN backend temporarily while running the forward pass of your RNN. For example: \nwith torch.backends.cudnn.flags(enabled=False):\n    output = model(inputs)"
     ]
    }
   ],
   "source": [
    "synthetic_text_data = torch.randint(0, len(vocab), (num_classes * num_synthetic_per_class, max_length), dtype=torch.float, device=device, requires_grad=True)\n",
    "synthetic_labels = torch.tensor([i for i in range(num_classes) for _ in range(num_synthetic_per_class)], dtype=torch.long, device=device)\n",
    "\n",
    "# Optimizer for synthetic data\n",
    "optimizer_syn = optim.SGD([synthetic_text_data], lr=0.01, momentum=0.9)\n",
    "\n",
    "def compute_gradients(model, inputs, labels):\n",
    "    outputs = model(inputs)\n",
    "    loss = criterion(outputs, labels)\n",
    "    gradients = torch.autograd.grad(loss, model.parameters(), create_graph=True)\n",
    "    return gradients\n",
    "\n",
    "def layerwise_matching_loss(gw_syn, gw_real):\n",
    "    loss = 0\n",
    "    for g_syn, g_real in zip(gw_syn, gw_real):\n",
    "        loss += ((g_syn - g_real) ** 2).sum()\n",
    "    return loss\n",
    "\n",
    "# Training loop with synthetic data gradient matching\n",
    "for iteration in range(num_iterations):\n",
    "    model.train()\n",
    "    loss_avg = 0\n",
    "\n",
    "    # Update synthetic data\n",
    "    for real_inputs, real_labels in train_loader:\n",
    "        real_inputs, real_labels = real_inputs.to(device), real_labels.to(device)\n",
    "\n",
    "        # Compute gradients for real data\n",
    "        gradients_real = compute_gradients(model, real_inputs, real_labels)\n",
    "        \n",
    "        # Compute gradients for synthetic data\n",
    "        synthetic_data_batch = synthetic_text_data[iteration % num_classes * num_synthetic_per_class: (iteration % num_classes + 1) * num_synthetic_per_class].long()\n",
    "        synthetic_labels_batch = synthetic_labels[iteration % num_classes * num_synthetic_per_class: (iteration % num_classes + 1) * num_synthetic_per_class]\n",
    "        gradients_synthetic = compute_gradients(model, synthetic_data_batch, synthetic_labels_batch)\n",
    "        \n",
    "        # Compute and minimize matching loss\n",
    "        loss_match = layerwise_matching_loss(gradients_synthetic, gradients_real)\n",
    "        optimizer_syn.zero_grad()\n",
    "        loss_match.backward()\n",
    "        optimizer_syn.step()\n",
    "        loss_avg += loss_match.item()\n",
    "\n",
    "    # Print loss every 10 iterations\n",
    "    if iteration % 10 == 0:\n",
    "        print(f\"Iteration {iteration}, Average Matching Loss: {loss_avg / len(train_loader):.4f}\")\n",
    "\n",
    "# Final evaluation on test set\n",
    "model.eval()\n",
    "test_labels = []\n",
    "test_preds = []\n",
    "with torch.no_grad():\n",
    "    for texts, labels in tqdm(test_loader, desc='Evaluating on test set', unit='batch'):\n",
    "        texts, labels = texts.to(device), labels.to(device)\n",
    "        outputs = model(texts)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        test_labels.extend(labels.cpu().numpy())\n",
    "        test_preds.extend(preds.cpu().numpy())\n",
    "\n",
    "overall_accuracy = accuracy_score(test_labels, test_preds)\n",
    "class_report = classification_report(test_labels, test_preds, target_names=['World', 'Sports', 'Business', 'Sci/Tech'])\n",
    "\n",
    "print(f'Test Accuracy: {overall_accuracy:.4f}')\n",
    "print('Classification Report:')\n",
    "print(class_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b880fd5-600b-49ae-8c91-a7c5858e365f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8094aa0c-d1ad-4705-a59d-1407e74d2920",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3efce1d6-5850-4f53-ac12-82999d77a69a",
   "metadata": {},
   "outputs": [],
   "source": [
    "synthetic_text_data = torch.randint(0, len(vocab), (num_classes * num_synthetic_per_class, max_length), dtype=torch.long, device=device, requires_grad=True)\n",
    "synthetic_labels = torch.tensor([i for i in range(num_classes) for _ in range(num_synthetic_per_class)], dtype=torch.long, device=device)\n",
    "\n",
    "# Optimizer for synthetic data\n",
    "optimizer_syn = optim.SGD([synthetic_text_data], lr=0.01, momentum=0.9)\n",
    "\n",
    "def compute_gradients(model, inputs, labels):\n",
    "    outputs = model(inputs)\n",
    "    loss = criterion(outputs, labels)\n",
    "    gradients = torch.autograd.grad(loss, model.parameters(), create_graph=True)\n",
    "    return gradients\n",
    "\n",
    "def layerwise_matching_loss(gw_syn, gw_real):\n",
    "    loss = 0\n",
    "    for g_syn, g_real in zip(gw_syn, gw_real):\n",
    "        loss += ((g_syn - g_real) ** 2).sum()\n",
    "    return loss\n",
    "\n",
    "# Training loop with synthetic data gradient matching\n",
    "for iteration in range(num_iterations):\n",
    "    model.train()\n",
    "    loss_avg = 0\n",
    "\n",
    "    # Update synthetic data\n",
    "    for real_inputs, real_labels in train_loader:\n",
    "        real_inputs, real_labels = real_inputs.to(device), real_labels.to(device)\n",
    "\n",
    "        # Compute gradients for real data\n",
    "        gradients_real = compute_gradients(model, real_inputs, real_labels)\n",
    "        \n",
    "        # Compute gradients for synthetic data\n",
    "        synthetic_data_batch = synthetic_text_data[iteration % num_classes * num_synthetic_per_class: (iteration % num_classes + 1) * num_synthetic_per_class]\n",
    "        synthetic_labels_batch = synthetic_labels[iteration % num_classes * num_synthetic_per_class: (iteration % num_classes + 1) * num_synthetic_per_class]\n",
    "        gradients_synthetic = compute_gradients(model, synthetic_data_batch, synthetic_labels_batch)\n",
    "        \n",
    "        # Compute and minimize matching loss\n",
    "        loss_match = layerwise_matching_loss(gradients_synthetic, gradients_real)\n",
    "        optimizer_syn.zero_grad()\n",
    "        loss_match.backward()\n",
    "        optimizer_syn.step()\n",
    "        loss_avg += loss_match.item()\n",
    "\n",
    "    # Print loss every 10 iterations\n",
    "    if iteration % 10 == 0:\n",
    "        print(f\"Iteration {iteration}, Average Matching Loss: {loss_avg / len(train_loader):.4f}\")\n",
    "\n",
    "# Final evaluation on test set\n",
    "model.eval()\n",
    "test_labels = []\n",
    "test_preds = []\n",
    "with torch.no_grad():\n",
    "    for texts, labels in tqdm(test_loader, desc='Evaluating on test set', unit='batch'):\n",
    "        texts, labels = texts.to(device), labels.to(device)\n",
    "        outputs = model(texts)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        test_labels.extend(labels.cpu().numpy())\n",
    "        test_preds.extend(preds.cpu().numpy())\n",
    "\n",
    "overall_accuracy = accuracy_score(test_labels, test_preds)\n",
    "class_report = classification_report(test_labels, test_preds, target_names=['World', 'Sports', 'Business', 'Sci/Tech'])\n",
    "\n",
    "print(f'Test Accuracy: {overall_accuracy:.4f}')\n",
    "print('Classification Report:')\n",
    "print(class_report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d9ef11-6237-45ef-a29d-f78d704ddf52",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2574cb0f-0ca8-484b-82c6-87c296a7f0ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff932629-f57a-47e2-bc49-38ee2e76c30e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "209cb8ae-47a4-4823-a7b1-5a8f3f7a3e40",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a668bcb7-7d10-4bf6-9780-6156cfe3eb19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d6e3c50a-95e5-4dfe-9c81-76b82aca2e13",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/IAIS/rrao/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset, random_split, TensorDataset\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from datasets import load_dataset\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# Load the Word2Vec model\n",
    "import gensim.downloader as api\n",
    "\n",
    "# Download the Word2Vec model\n",
    "word2vec_model = api.load('word2vec-google-news-300')\n",
    "\n",
    "# Download NLTK data\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Load the AG News dataset\n",
    "dataset = load_dataset(\"ag_news\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a6f494-fe1c-4fab-ba63-be5b413aa7ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "723eb365-3103-40dc-8422-6786e8b66eed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28211230-cbe1-4b10-a29a-271010ebed1f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bebe19f-244b-4925-8bc2-9ffaf1c7a51c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0861c2f-4a2b-40b4-ab92-c5129acb134f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e9dd7212-f298-447f-9bf4-166996a79b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLTK Tokenizer Function\n",
    "def nltk_tokenizer(text):\n",
    "    return word_tokenize(text.lower())\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "class AGNewsDataset(Dataset):\n",
    "    def __init__(self, texts, labels, vocab, max_length):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.vocab = vocab\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        tokenized_text = [self.vocab.get(token, self.vocab['<UNK>']) for token in text]\n",
    "        if len(tokenized_text) < self.max_length:\n",
    "            tokenized_text += [self.vocab['<PAD>']] * (self.max_length - len(tokenized_text))\n",
    "        else:\n",
    "            tokenized_text = tokenized_text[:self.max_length]\n",
    "        return torch.tensor(tokenized_text, dtype=torch.long), torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "# Build vocabulary\n",
    "def build_vocab(dataset, tokenizer):\n",
    "    vocab = {'<PAD>': 0, '<UNK>': 1}\n",
    "    for example in dataset:\n",
    "        tokens = tokenizer(example['text'])\n",
    "        for token in tokens:\n",
    "            if token not in vocab:\n",
    "                vocab[token] = len(vocab)\n",
    "    return vocab\n",
    "\n",
    "# Tokenize and build vocab\n",
    "train_texts = [nltk_tokenizer(example['text']) for example in dataset['train']]\n",
    "train_labels = [example['label'] for example in dataset['train']]\n",
    "vocab = build_vocab(dataset['train'], nltk_tokenizer)\n",
    "\n",
    "# Set max length for padding\n",
    "max_length = 128\n",
    "\n",
    "# Create dataset\n",
    "full_dataset = AGNewsDataset(train_texts, train_labels, vocab, max_length)\n",
    "\n",
    "# Split training set into training and validation sets\n",
    "train_size = int(0.8 * len(full_dataset))\n",
    "val_size = len(full_dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])\n",
    "\n",
    "# Prepare test dataset\n",
    "test_texts = [nltk_tokenizer(example['text']) for example in dataset['test']]\n",
    "test_labels = [example['label'] for example in dataset['test']]\n",
    "test_dataset = AGNewsDataset(test_texts, test_labels, vocab, max_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2ae7ca65-4d43-4fb4-9e7e-92f83b5c504e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1237708/2929804843.py:51: DeprecationWarning: an integer is required (got type numpy.float64).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  synthetic_labels = torch.tensor([np.ones(num_synthetic_per_class) * i for i in range(num_classes)], dtype=torch.long, device=device).view(-1)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fcb680ba-74e7-44f2-a7f1-e3f9aaf52742",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                              | 0/1500 [00:02<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got torch.cuda.FloatTensor instead (while checking arguments for embedding)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 15\u001b[0m\n\u001b[1;32m     13\u001b[0m synthetic_data_batch \u001b[38;5;241m=\u001b[39m synthetic_text_data[iteration \u001b[38;5;241m%\u001b[39m num_classes \u001b[38;5;241m*\u001b[39m num_synthetic_per_class: (iteration \u001b[38;5;241m%\u001b[39m num_classes \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m*\u001b[39m num_synthetic_per_class]\n\u001b[1;32m     14\u001b[0m synthetic_labels_batch \u001b[38;5;241m=\u001b[39m synthetic_labels[iteration \u001b[38;5;241m%\u001b[39m num_classes \u001b[38;5;241m*\u001b[39m num_synthetic_per_class: (iteration \u001b[38;5;241m%\u001b[39m num_classes \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m*\u001b[39m num_synthetic_per_class]\n\u001b[0;32m---> 15\u001b[0m gradients_synthetic \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_gradients\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msynthetic_data_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msynthetic_labels_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Compute and minimize matching loss\u001b[39;00m\n\u001b[1;32m     18\u001b[0m loss_match \u001b[38;5;241m=\u001b[39m layerwise_matching_loss(gradients_synthetic, gradients_real)\n",
      "Cell \u001b[0;32mIn[17], line 2\u001b[0m, in \u001b[0;36mcompute_gradients\u001b[0;34m(model, inputs, labels)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_gradients\u001b[39m(model, inputs, labels):\n\u001b[0;32m----> 2\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[1;32m      4\u001b[0m     gradients \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mgrad(loss, model\u001b[38;5;241m.\u001b[39mparameters(), create_graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[16], line 23\u001b[0m, in \u001b[0;36mDeepLSTMModel.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 23\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m     output, (hidden, cell) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlstm(x)\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc(hidden[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/sparse.py:162\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 162\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/functional.py:2210\u001b[0m, in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2204\u001b[0m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[1;32m   2205\u001b[0m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[1;32m   2206\u001b[0m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[1;32m   2207\u001b[0m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[1;32m   2208\u001b[0m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[1;32m   2209\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[0;32m-> 2210\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got torch.cuda.FloatTensor instead (while checking arguments for embedding)"
     ]
    }
   ],
   "source": [
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Training loop with synthetic data gradient matching\n",
    "for iteration in range(num_iterations):\n",
    "    model.train()\n",
    "    for real_inputs, real_labels in tqdm(train_loader):\n",
    "        real_inputs, real_labels = real_inputs.to(device), real_labels.to(device)\n",
    "\n",
    "        # Compute gradients for real data\n",
    "        gradients_real = compute_gradients(model, real_inputs, real_labels)\n",
    "        \n",
    "        # Compute gradients for synthetic data\n",
    "        synthetic_data_batch = synthetic_text_data[iteration % num_classes * num_synthetic_per_class: (iteration % num_classes + 1) * num_synthetic_per_class]\n",
    "        synthetic_labels_batch = synthetic_labels[iteration % num_classes * num_synthetic_per_class: (iteration % num_classes + 1) * num_synthetic_per_class]\n",
    "        gradients_synthetic = compute_gradients(model, synthetic_data_batch, synthetic_labels_batch)\n",
    "        \n",
    "        # Compute and minimize matching loss\n",
    "        loss_match = layerwise_matching_loss(gradients_synthetic, gradients_real)\n",
    "        optimizer_syn.zero_grad()\n",
    "        loss_match.backward()\n",
    "        optimizer_syn.step()\n",
    "\n",
    "    print(f\"Iteration {iteration}, Matching Loss: {loss_match.item()}\")\n",
    "\n",
    "# Final evaluation on test set\n",
    "model.eval()\n",
    "test_labels = []\n",
    "test_preds = []\n",
    "with torch.no_grad():\n",
    "    for texts, labels in tqdm(test_loader, desc='Evaluating on test set', unit='batch'):\n",
    "        texts, labels = texts.to(device), labels.to(device)\n",
    "        outputs = model(texts)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        test_labels.extend(labels.cpu().numpy())\n",
    "        test_preds.extend(preds.cpu().numpy())\n",
    "\n",
    "overall_accuracy = accuracy_score(test_labels, test_preds)\n",
    "class_report = classification_report(test_labels, test_preds, target_names=['World', 'Sports', 'Business', 'Sci/Tech'])\n",
    "\n",
    "print(f'Test Accuracy: {overall_accuracy:.4f}')\n",
    "print('Classification Report:')\n",
    "print(class_report)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
