{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b491accc-2732-43f2-b45e-dc1974fd251b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1875/1875 [02:19<00:00, 13.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Train Loss: 912.2557, Train Accuracy: 83.22%, Test Accuracy: 87.80%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1875/1875 [02:17<00:00, 13.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/10], Train Loss: 675.8879, Train Accuracy: 87.84%, Test Accuracy: 88.41%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1875/1875 [02:16<00:00, 13.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/10], Train Loss: 643.8324, Train Accuracy: 88.39%, Test Accuracy: 88.46%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1875/1875 [02:16<00:00, 13.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/10], Train Loss: 623.2161, Train Accuracy: 88.72%, Test Accuracy: 88.54%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1875/1875 [02:18<00:00, 13.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/10], Train Loss: 610.8825, Train Accuracy: 88.91%, Test Accuracy: 88.43%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1875/1875 [02:18<00:00, 13.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/10], Train Loss: 600.3323, Train Accuracy: 89.05%, Test Accuracy: 88.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1875/1875 [02:16<00:00, 13.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/10], Train Loss: 588.4040, Train Accuracy: 89.24%, Test Accuracy: 88.33%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1875/1875 [02:19<00:00, 13.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/10], Train Loss: 578.3513, Train Accuracy: 89.44%, Test Accuracy: 88.46%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1875/1875 [02:18<00:00, 13.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/10], Train Loss: 573.1492, Train Accuracy: 89.40%, Test Accuracy: 88.91%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1875/1875 [02:16<00:00, 13.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/10], Train Loss: 566.3935, Train Accuracy: 89.59%, Test Accuracy: 88.67%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       World       0.93      0.87      0.90      1900\n",
      "      Sports       0.94      0.97      0.96      1900\n",
      "    Business       0.83      0.86      0.84      1900\n",
      "    Sci/Tech       0.86      0.86      0.86      1900\n",
      "\n",
      "    accuracy                           0.89      7600\n",
      "   macro avg       0.89      0.89      0.89      7600\n",
      "weighted avg       0.89      0.89      0.89      7600\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "from tqdm import tqdm\n",
    "from transformers import BertTokenizer\n",
    "from gensim.models import KeyedVectors\n",
    "from datasets import load_dataset\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "import gensim.downloader as api\n",
    "\n",
    "# Load AG News Dataset\n",
    "dataset = load_dataset('ag_news')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def tokenize(text):\n",
    "    return tokenizer(text, padding='max_length', truncation=True, return_tensors='pt', max_length=128)\n",
    "\n",
    "# Load Word2Vec Embeddings\n",
    "word2vec = api.load('word2vec-google-news-300')\n",
    "def get_word2vec_embedding(tokens):\n",
    "    embeddings = []\n",
    "    for token in tokens:\n",
    "        if token in word2vec:\n",
    "            embeddings.append(word2vec[token])\n",
    "    if len(embeddings) == 0:\n",
    "        return np.zeros(word2vec.vector_size)\n",
    "    return np.mean(embeddings, axis=0)\n",
    "\n",
    "class AGNewsDataset(data.Dataset):\n",
    "    def __init__(self, texts, labels):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        tokens = tokenizer.tokenize(self.texts[idx])\n",
    "        embedding = get_word2vec_embedding(tokens)\n",
    "        label = self.labels[idx]\n",
    "        return torch.tensor(embedding, dtype=torch.float32), torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "train_texts = [item['text'] for item in dataset['train']]\n",
    "train_labels = [item['label'] for item in dataset['train']]\n",
    "test_texts = [item['text'] for item in dataset['test']]\n",
    "test_labels = [item['label'] for item in dataset['test']]\n",
    "\n",
    "train_dataset = AGNewsDataset(train_texts, train_labels)\n",
    "test_dataset = AGNewsDataset(test_texts, test_labels)\n",
    "\n",
    "train_loader = data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = data.DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "class CNNModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNModel, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(1, 100, kernel_size=3, padding=1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.pool = nn.MaxPool1d(2)\n",
    "        self.fc1 = nn.Linear(100 * (word2vec.vector_size // 2), 4)  # 4 classes in AG News\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1)  # Add channel dimension\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc1(x)\n",
    "        return x\n",
    "\n",
    "model = CNNModel()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training and Evaluation\n",
    "num_epochs = 10\n",
    "best_accuracy = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for inputs, labels in tqdm(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    train_accuracy = 100 * correct / total\n",
    "    \n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    test_accuracy = 100 * correct / total\n",
    "    print(f'Epoch [{epoch + 1}/{num_epochs}], Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.2f}%, Test Accuracy: {test_accuracy:.2f}%')\n",
    "    \n",
    "    if test_accuracy > best_accuracy:\n",
    "        best_accuracy = test_accuracy\n",
    "        torch.save(model.state_dict(), 'best_model.pt')\n",
    "\n",
    "# Load the best model\n",
    "model.load_state_dict(torch.load('best_model.pt'))\n",
    "\n",
    "# Generate Classification Report\n",
    "y_true = []\n",
    "y_pred = []\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        y_true.extend(labels.tolist())\n",
    "        y_pred.extend(predicted.tolist())\n",
    "\n",
    "print(classification_report(y_true, y_pred, target_names=dataset['test'].features['label'].names))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f8623bd-6896-4833-a3e0-6db7f5e95e27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31719893-45ee-4138-9462-8db6da871cb8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c87a5d-ccbb-4ced-a62c-8d50ef6289ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c340d903-2e8f-4c22-b53c-1fc6f233a165",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6156b6d6-cc4b-4324-bbfb-d472e00bdd94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "38e96aac-4299-4961-9f3f-d7169c9f38ed",
   "metadata": {},
   "source": [
    "## OLD CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d5704687-cbc2-44ba-8305-b3441ef65c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer\n",
    "import numpy as np\n",
    "\n",
    "# Load the Word2Vec model\n",
    "word2vec_model = api.load('word2vec-google-news-300')\n",
    "\n",
    "# Load the AG News dataset\n",
    "dataset = load_dataset(\"ag_news\")\n",
    "\n",
    "# Load tokenizer from HuggingFace\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "class AGNewsDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        return encoding['input_ids'].squeeze(), encoding['attention_mask'].squeeze(), torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "# Tokenize and prepare datasets\n",
    "def prepare_dataset(dataset, tokenizer, max_length):\n",
    "    texts = [example['text'] for example in dataset]\n",
    "    labels = [example['label'] for example in dataset]\n",
    "    return AGNewsDataset(texts, labels, tokenizer, max_length)\n",
    "\n",
    "# Set max length for padding\n",
    "max_length = 128\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = prepare_dataset(dataset['train'], tokenizer, max_length)\n",
    "test_dataset = prepare_dataset(dataset['test'], tokenizer, max_length)\n",
    "\n",
    "# Split training set into training and validation sets\n",
    "train_size = int(0.8 * len(train_dataset))\n",
    "val_size = len(train_dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(train_dataset, [train_size, val_size])\n",
    "\n",
    "class CNNModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, output_dim, embedding_matrix):\n",
    "        super(CNNModel, self).__init__()\n",
    "        self.embedding = nn.Embedding.from_pretrained(embedding_matrix, freeze=False)\n",
    "        self.conv1 = nn.Conv2d(1, 100, (3, embedding_dim))\n",
    "        self.conv2 = nn.Conv2d(1, 100, (4, embedding_dim))\n",
    "        self.conv3 = nn.Conv2d(1, 100, (5, embedding_dim))\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc = nn.Linear(300, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x).unsqueeze(1)  # Add channel dimension\n",
    "        x1 = torch.relu(self.conv1(x)).squeeze(3)\n",
    "        x1 = torch.max_pool1d(x1, x1.size(2)).squeeze(2)\n",
    "        x2 = torch.relu(self.conv2(x)).squeeze(3)\n",
    "        x2 = torch.max_pool1d(x2, x2.size(2)).squeeze(2)\n",
    "        x3 = torch.relu(self.conv3(x)).squeeze(3)\n",
    "        x3 = torch.max_pool1d(x3, x3.size(2)).squeeze(2)\n",
    "        x = torch.cat((x1, x2, x3), 1)\n",
    "        x = self.dropout(x)\n",
    "        return self.fc(x)\n",
    "\n",
    "# Training Parameters\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 10\n",
    "OUTPUT_DIM = 4\n",
    "LR = 0.001\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# DataLoader\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e0ac463-4917-4fbc-846d-db995649a9bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ce31ba9-8b08-4aa6-bf87-35748054d962",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   0%|          | 0/1500 [00:26<?, ?batch/s]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 3.35 GiB (GPU 0; 11.92 GiB total capacity; 10.08 GiB already allocated; 1.69 GiB free; 10.08 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 56\u001b[0m\n\u001b[1;32m     54\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     55\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 56\u001b[0m     \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m     59\u001b[0m val_labels \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/optim/optimizer.py:280\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    276\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    277\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs),\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    278\u001b[0m                                \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 280\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    281\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    283\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/optim/optimizer.py:33\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     32\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m---> 33\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     35\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(prev_grad)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/optim/adam.py:132\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    129\u001b[0m     state_steps \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    130\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m--> 132\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_init_group\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    133\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    134\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    135\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    136\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    137\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    138\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    139\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    141\u001b[0m     adam(\n\u001b[1;32m    142\u001b[0m         params_with_grad,\n\u001b[1;32m    143\u001b[0m         grads,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    160\u001b[0m         found_inf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_inf\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m    161\u001b[0m     )\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/optim/adam.py:94\u001b[0m, in \u001b[0;36mAdam._init_group\u001b[0;34m(self, group, params_with_grad, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps)\u001b[0m\n\u001b[1;32m     92\u001b[0m state[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexp_avg\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros_like(p, memory_format\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mpreserve_format)\n\u001b[1;32m     93\u001b[0m \u001b[38;5;66;03m# Exponential moving average of squared gradient values\u001b[39;00m\n\u001b[0;32m---> 94\u001b[0m state[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexp_avg_sq\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzeros_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemory_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreserve_format\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m group[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mamsgrad\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;66;03m# Maintains max of all exp. moving avg. of sq. grad. values\u001b[39;00m\n\u001b[1;32m     97\u001b[0m     state[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_exp_avg_sq\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros_like(p, memory_format\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mpreserve_format)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 3.35 GiB (GPU 0; 11.92 GiB total capacity; 10.08 GiB already allocated; 1.69 GiB free; 10.08 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import contextlib\n",
    "import sys\n",
    "\n",
    "# Assuming the necessary variables are defined: word2vec_model, CNNModel, device, OUTPUT_DIM, LR, EPOCHS, train_loader, val_loader, test_loader\n",
    "\n",
    "class Logger:\n",
    "    def __init__(self, filename):\n",
    "        self.terminal = sys.stdout\n",
    "        self.log = open(filename, 'w')\n",
    "\n",
    "    def write(self, message):\n",
    "        self.terminal.write(message)\n",
    "        self.log.write(message)\n",
    "        self.log.flush()\n",
    "\n",
    "    def flush(self):\n",
    "        self.terminal.flush()\n",
    "        self.log.flush()\n",
    "\n",
    "sys.stdout = Logger('output_log.txt')\n",
    "sys.stderr = Logger('output_log.txt')\n",
    "\n",
    "# Initialize embedding matrix\n",
    "def build_embedding_matrix(word2vec_model, embedding_dim):\n",
    "    embedding_matrix = np.zeros((len(word2vec_model), embedding_dim))\n",
    "    for i, word in enumerate(word2vec_model.index_to_key):\n",
    "        embedding_matrix[i] = word2vec_model[word]\n",
    "    return torch.tensor(embedding_matrix, dtype=torch.float32)\n",
    "\n",
    "# Build the embedding matrix\n",
    "embedding_dim = 300  # Word2Vec uses 300-dimensional vectors\n",
    "embedding_matrix = build_embedding_matrix(word2vec_model, embedding_dim)\n",
    "\n",
    "# Model, loss function, and optimizer\n",
    "model = CNNModel(len(word2vec_model), embedding_dim, OUTPUT_DIM, embedding_matrix).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    for texts, _, labels in tqdm(train_loader, desc=f'Epoch {epoch + 1}/{EPOCHS}', unit='batch'):\n",
    "        texts, labels = texts.to(device), labels.to(device)\n",
    "\n",
    "        outputs = model(texts)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "    val_labels = []\n",
    "    val_preds = []\n",
    "    with torch.no_grad():\n",
    "        for texts, _, labels in tqdm(val_loader, desc=f'Epoch {epoch + 1}/{EPOCHS}', unit='batch'):\n",
    "            texts, labels = texts.to(device), labels.to(device)\n",
    "            outputs = model(texts)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            val_labels.extend(labels.cpu().numpy())\n",
    "            val_preds.extend(preds.cpu().numpy())\n",
    "\n",
    "    val_accuracy = accuracy_score(val_labels, val_preds)\n",
    "    print(f'Epoch {epoch + 1}/{EPOCHS}, Validation Accuracy: {val_accuracy:.4f}')\n",
    "\n",
    "# Final evaluation on test set\n",
    "model.eval()\n",
    "test_labels = []\n",
    "test_preds = []\n",
    "with torch.no_grad():\n",
    "    for texts, _, labels in tqdm(test_loader, desc='Testing', unit='batch'):\n",
    "        texts, labels = texts.to(device), labels.to(device)\n",
    "        outputs = model(texts)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        test_labels.extend(labels.cpu().numpy())\n",
    "        test_preds.extend(preds.cpu().numpy())\n",
    "\n",
    "overall_accuracy = accuracy_score(test_labels, test_preds)\n",
    "class_report = classification_report(test_labels, test_preds, target_names=['World', 'Sports', 'Business', 'Sci/Tech'])\n",
    "\n",
    "print(f'Test Accuracy: {overall_accuracy:.4f}')\n",
    "print('Classification Report:')\n",
    "print(class_report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa2ac4cd-5e11-4ce4-8e0b-2fb80b0a7ba1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db32dd2a-73cb-4d60-9ae3-e97034f9501f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd1115cb-40b9-4d00-be89-3dfd6d2def75",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "643a026b-3663-474f-9125-1a32614bf012",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f1233c7e-40d2-4adb-bd68-3ecad35ec6b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   0%|                                                                                                                               | 0/1500 [00:25<?, ?batch/s]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 3.35 GiB (GPU 0; 11.92 GiB total capacity; 3.38 GiB already allocated; 1.59 GiB free; 3.40 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 28\u001b[0m\n\u001b[1;32m     25\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[1;32m     27\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 28\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     31\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 3.35 GiB (GPU 0; 11.92 GiB total capacity; 3.38 GiB already allocated; 1.59 GiB free; 3.40 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# Initialize embedding matrix\n",
    "def build_embedding_matrix(word2vec_model, embedding_dim):\n",
    "    embedding_matrix = np.zeros((len(word2vec_model), embedding_dim))\n",
    "    for i, word in enumerate(word2vec_model.index_to_key):\n",
    "        embedding_matrix[i] = word2vec_model[word]\n",
    "    return torch.tensor(embedding_matrix, dtype=torch.float32)\n",
    "\n",
    "# Build the embedding matrix\n",
    "embedding_dim = 300  # Word2Vec uses 300-dimensional vectors\n",
    "embedding_matrix = build_embedding_matrix(word2vec_model, embedding_dim)\n",
    "\n",
    "# Model, loss function, and optimizer\n",
    "model = CNNModel(len(word2vec_model), embedding_dim, OUTPUT_DIM, embedding_matrix).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    for texts, _, labels in tqdm(train_loader, desc=f'Epoch {epoch + 1}/{EPOCHS}', unit='batch'):\n",
    "        texts, labels = texts.to(device), labels.to(device)\n",
    "        \n",
    "        outputs = model(texts)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    model.eval()\n",
    "    val_labels = []\n",
    "    val_preds = []\n",
    "    with torch.no_grad():\n",
    "        for texts, _, labels in tqdm(val_loader, desc=f'Epoch {epoch + 1}/{EPOCHS}', unit='batch'):\n",
    "            texts, labels = texts.to(device), labels.to(device)\n",
    "            outputs = model(texts)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            val_labels.extend(labels.cpu().numpy())\n",
    "            val_preds.extend(preds.cpu().numpy())\n",
    "    \n",
    "    val_accuracy = accuracy_score(val_labels, val_preds)\n",
    "    print(f'Epoch {epoch + 1}/{EPOCHS}, Validation Accuracy: {val_accuracy:.4f}')\n",
    "\n",
    "# Final evaluation on test set\n",
    "model.eval()\n",
    "test_labels = []\n",
    "test_preds = []\n",
    "with torch.no_grad():\n",
    "    for texts, _, labels in tqdm(test_loader, desc='Testing', unit='batch'):\n",
    "        texts, labels = texts.to(device), labels.to(device)\n",
    "        outputs = model(texts)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        test_labels.extend(labels.cpu().numpy())\n",
    "        test_preds.extend(preds.cpu().numpy())\n",
    "\n",
    "overall_accuracy = accuracy_score(test_labels, test_preds)\n",
    "class_report = classification_report(test_labels, test_preds, target_names=['World', 'Sports', 'Business', 'Sci/Tech'])\n",
    "\n",
    "print(f'Test Accuracy: {overall_accuracy:.4f}')\n",
    "print('Classification Report:')\n",
    "print(class_report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "013f0540-85f3-4624-b89e-ef8ab6bc2420",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b775ac0b-e0b0-4aee-b7cd-4b9211b219b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b65294-b813-4fa3-b57b-93a1908d272f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba304230-bc34-4391-bc2f-a22fc5df0b97",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a78840-27ca-4a25-b45a-79a091e567ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/IAIS/rrao/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "Epoch 1/10:   2%|██▎                                                                                                                | 30/1500 [14:04<11:54:50, 29.18s/batch]"
     ]
    }
   ],
   "source": [
    "import gensim.downloader as api\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from datasets import load_dataset\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Download NLTK data\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Download the Word2Vec model\n",
    "word2vec_model = api.load('word2vec-google-news-300')\n",
    "\n",
    "# Load the AG News dataset\n",
    "dataset = load_dataset(\"ag_news\")\n",
    "\n",
    "# NLTK Tokenizer Function\n",
    "def nltk_tokenizer(text):\n",
    "    return word_tokenize(text.lower())\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "class AGNewsDataset(Dataset):\n",
    "    def __init__(self, texts, labels, vocab, max_length):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.vocab = vocab\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        tokenized_text = [self.vocab.get(token, self.vocab['<UNK>']) for token in text]\n",
    "        if len(tokenized_text) < self.max_length:\n",
    "            tokenized_text += [self.vocab['<PAD>']] * (self.max_length - len(tokenized_text))\n",
    "        else:\n",
    "            tokenized_text = tokenized_text[:self.max_length]\n",
    "        return torch.tensor(tokenized_text, dtype=torch.long), torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "# Build vocabulary\n",
    "def build_vocab(dataset, tokenizer):\n",
    "    vocab = {'<PAD>': 0, '<UNK>': 1}\n",
    "    for example in dataset:\n",
    "        tokens = tokenizer(example['text'])\n",
    "        for token in tokens:\n",
    "            if token not in vocab:\n",
    "                vocab[token] = len(vocab)\n",
    "    return vocab\n",
    "\n",
    "# Tokenize and build vocab\n",
    "train_texts = [nltk_tokenizer(example['text']) for example in dataset['train']]\n",
    "train_labels = [example['label'] for example in dataset['train']]\n",
    "vocab = build_vocab(dataset['train'], nltk_tokenizer)\n",
    "\n",
    "# Set max length for padding\n",
    "max_length = 128\n",
    "\n",
    "# Create dataset\n",
    "full_dataset = AGNewsDataset(train_texts, train_labels, vocab, max_length)\n",
    "\n",
    "# Split training set into training and validation sets\n",
    "train_size = int(0.8 * len(full_dataset))\n",
    "val_size = len(full_dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])\n",
    "\n",
    "# Prepare test dataset\n",
    "test_texts = [nltk_tokenizer(example['text']) for example in dataset['test']]\n",
    "test_labels = [example['label'] for example in dataset['test']]\n",
    "test_dataset = AGNewsDataset(test_texts, test_labels, vocab, max_length)\n",
    "\n",
    "# Initialize embedding matrix\n",
    "def build_embedding_matrix(vocab, word2vec_model, embedding_dim):\n",
    "    embedding_matrix = np.zeros((len(vocab), embedding_dim))\n",
    "    for word, idx in vocab.items():\n",
    "        if word in word2vec_model:\n",
    "            embedding_matrix[idx] = word2vec_model[word]\n",
    "        else:\n",
    "            embedding_matrix[idx] = np.random.normal(size=(embedding_dim,))\n",
    "    return torch.tensor(embedding_matrix, dtype=torch.float32)\n",
    "\n",
    "# Build the embedding matrix\n",
    "embedding_dim = 300  # Word2Vec uses 300-dimensional vectors\n",
    "embedding_matrix = build_embedding_matrix(vocab, word2vec_model, embedding_dim)\n",
    "\n",
    "class CNNModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, output_dim, embedding_matrix):\n",
    "        super(CNNModel, self).__init__()\n",
    "        self.embedding = nn.Embedding.from_pretrained(embedding_matrix, freeze=False)\n",
    "        self.conv1 = nn.Conv2d(1, 100, (3, embedding_dim))\n",
    "        self.conv2 = nn.Conv2d(1, 100, (4, embedding_dim))\n",
    "        self.conv3 = nn.Conv2d(1, 100, (5, embedding_dim))\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc = nn.Linear(300, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x).unsqueeze(1)  # Add channel dimension\n",
    "        x1 = torch.relu(self.conv1(x)).squeeze(3)\n",
    "        x1 = torch.max_pool1d(x1, x1.size(2)).squeeze(2)\n",
    "        x2 = torch.relu(self.conv2(x)).squeeze(3)\n",
    "        x2 = torch.max_pool1d(x2, x2.size(2)).squeeze(2)\n",
    "        x3 = torch.relu(self.conv3(x)).squeeze(3)\n",
    "        x3 = torch.max_pool1d(x3, x3.size(2)).squeeze(2)\n",
    "        x = torch.cat((x1, x2, x3), 1)\n",
    "        x = self.dropout(x)\n",
    "        return self.fc(x)\n",
    "\n",
    "# Training Parameters\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 10\n",
    "OUTPUT_DIM = 4\n",
    "LR = 0.001\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# DataLoader\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# Model, loss function, and optimizer\n",
    "model = CNNModel(len(vocab), embedding_dim, OUTPUT_DIM, embedding_matrix).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    for texts, labels in tqdm(train_loader, desc=f'Epoch {epoch + 1}/{EPOCHS}', unit='batch'):\n",
    "        texts, labels = texts.to(device), labels.to(device)\n",
    "        \n",
    "        outputs = model(texts)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    model.eval()\n",
    "    val_labels = []\n",
    "    val_preds = []\n",
    "    with torch.no_grad():\n",
    "        for texts, labels in tqdm(val_loader, desc=f'Epoch {epoch + 1}/{EPOCHS}', unit='batch'):\n",
    "            texts, labels = texts.to(device), labels.to(device)\n",
    "            outputs = model(texts)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            val_labels.extend(labels.cpu().numpy())\n",
    "            val_preds.extend(preds.cpu().numpy())\n",
    "    \n",
    "    val_accuracy = accuracy_score(val_labels, val_preds)\n",
    "    print(f'Epoch {epoch + 1}/{EPOCHS}, Validation Accuracy: {val_accuracy:.4f}')\n",
    "\n",
    "# Final evaluation on test set\n",
    "model.eval()\n",
    "test_labels = []\n",
    "test_preds = []\n",
    "with torch.no_grad():\n",
    "    for texts, labels in tqdm(test_loader, desc='Testing', unit='batch'):\n",
    "        texts, labels = texts.to(device), labels.to(device)\n",
    "        outputs = model(texts)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        test_labels.extend(labels.cpu().numpy())\n",
    "        test_preds.extend(preds.cpu().numpy())\n",
    "\n",
    "overall_accuracy = accuracy_score(test_labels, test_preds)\n",
    "class_report = classification_report(test_labels, test_preds, target_names=['World', 'Sports', 'Business', 'Sci/Tech'])\n",
    "\n",
    "print(f'Test Accuracy: {overall_accuracy:.4f}')\n",
    "print('Classification Report:')\n",
    "print(class_report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "227f609e-a4cd-4130-98b4-71bbf2f37d4e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
